{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(1337) # for reproducibility\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.layers.core import Lambda\nfrom keras.engine.topology import Layer\nfrom keras.models import Sequential\nfrom keras.layers import Input,Dense,GRU,LSTM,Concatenate,Dropout,Activation,Add, Masking\nfrom keras.layers.pooling import AveragePooling1D,MaxPooling1D\nfrom keras.layers.core import Flatten\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers.convolutional import Conv1D\nfrom keras.models import Model\nfrom keras.layers.wrappers import TimeDistributed, Bidirectional\nfrom keras.layers.core import Reshape\nfrom keras.backend import shape\nfrom keras.utils import plot_model\nfrom keras.layers.merge import Multiply,Concatenate\nimport pickle\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom keras.optimizers import RMSprop,Adadelta,Adam\nfrom keras.callbacks import Callback\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_test_result(result, test_label):\n    #print(result)\n    true_label=[]\n    predicted_label=[]\n\n    for i in range(result.shape[0]):\n        for j in range(result.shape[1]):\n            if np.argmax(test_label[i,j] )!=0 and np.argmax(result[i,j] )!=0:\n                true_label.append(np.argmax(test_label[i,j] ))\n                predicted_label.append(np.argmax(result[i,j] ))\n    \n    print(\"Confusion Matrix :\")\n    print(confusion_matrix(true_label, predicted_label))\n    print(\"Classification Report :\")\n    print(classification_report(true_label, predicted_label,digits=4))\n    print(\"Accuracy \", accuracy_score(true_label, predicted_label))\n    print(\"Macro Classification Report :\")\n    print(precision_recall_fscore_support(true_label, predicted_label,average='macro'))\n    print(\"Weighted Classification Report :\")\n    print(precision_recall_fscore_support(true_label, predicted_label,average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"NUM_LABELS=3\ndim_proj=300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('../input/mosei-dataset-3way/text_3way.pickle', 'rb') as handle:\n#     text_activations = pickle.load(handle, encoding = 'latin1')\n\n# print(len(text_activations))\n# print(len(text_activations[11]))\n# l1 = text_activations[1]\n# ind = (l1[:, :, 0]!=1)\n# print(l1)\n# print(ind)\n# l2 = l1[ind]\n# print(l2)\n\n# with open('../input/mosei-dataset-3way/audio_3way.pickle', 'rb') as handle:\n#     audio_activations = pickle.load(handle, encoding = 'latin1')\n\n# # print(audio_activations)\n# l2 = audio_activations[1]\n\n# print(l1==l2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Bimodal():\n    with open('../input/mosei-dataset-3way/text_3way.pickle', 'rb') as handle:\n        text_activations = pickle.load(handle, encoding = 'latin1')\n    \n    with open('../input/mosei-dataset-3way/audio_3way.pickle', 'rb') as handle:\n        audio_activations = pickle.load(handle, encoding = 'latin1')\n        \n#     N = len(text_activations)//2\n    \n    train_data_t = text_activations[0]\n#     train_data_t = np.concatenate([train_data_t, text_activations[4]], 0)\n    train_data_a = audio_activations[0]\n#     train_data_a = np.concatenate([train_data_a, audio_activations[4]], 0)\n    test_data_t = text_activations[2]\n    test_data_a = audio_activations[2]\n    train_label = text_activations[1]\n#     train_label = np.concatenate([train_label, text_activations[5]], 0)\n    test_label = text_activations[3]\n    \n    train_label_i=np.argmax(train_label,axis=-1)\n    test_label_i=np.argmax(test_label,axis=-1)\n    \n    for i in range(train_data_t.shape[0]):\n        for j in range(train_data_t.shape[1]):\n            if train_label_i[i][j] == 0 :\n                train_data_t[i,j,:]=0.0\n                train_data_a[i,j,:]=0.0\n                \n    for i in range(test_data_t.shape[0]):\n        for j in range(test_data_t.shape[1]):\n            if test_label_i[i][j] == 0 :\n                test_data_t[i,j,:]=0.0\n                test_data_a[i,j,:]=0.0\n    \n#     train_label = train_label_i-1\n#     test_label = test_label_i-1\n\n#     for i in range(unimodal_activations['audio_test'].shape[0]):\n#         for j in range(unimodal_activations['audio_test'].shape[1]):\n#             if test_mask[i][j] == 0.0 :\n#                 merged_test_data[i,j,:]=0.0]\n        \n    print(train_data_t.shape)\n    print(train_data_a.shape)\n    print(test_data_t.shape)\n    print(train_label.shape)\n    print(test_label.shape)\n    \n    dim_1 = train_data_t.shape[0]\n    max_len = train_data_t.shape[1]\n    dim_text = train_data_t.shape[2]\n    dim_aud = train_data_a.shape[2]\n    \n    dimt_1 = test_data_t.shape[0]\n    maxt_len = test_data_t.shape[1]\n    \n    \n    x=Input(shape=(max_len,dim_text,), name='bi_t_input')\n    y=Input(shape=(max_len,dim_aud,), name='bi_a_input')\n    x1=Masking(mask_value =0.0 , name='bi_mask_t')(x)\n    y1=Masking(mask_value =0.0 , name='bi_mask_a')(y)\n#     sg1=GRU(400, return_sequences=True, dropout=0.2, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3))(x1)\n#     sg2=GRU(200, return_sequences=True, dropout=0.2, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3))(y1)\n    d1=Dense(200, activation='tanh', use_bias=True, trainable=True, name='bid1')(x1)\n    d2=Dense(100, activation='tanh', use_bias=True, trainable=True, name='bid2')(y1)\n    c1=Concatenate(axis=-1)([d1, d2])\n    g1=GRU(300, return_sequences=True, dropout=0.2, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3))(c1)\n    d3=TimeDistributed(Dense(NUM_LABELS,activation='softmax',kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3)))(g1)\n    \n    model=Model([x, y], outputs=d3)\n    model.compile(loss='categorical_crossentropy',optimizer=Adam(0.0001),metrics=['accuracy'])\n    print(model.summary())\n    \n#     model.fit([train_data_t, train_data_a], train_label, epochs=5, batch_size=20, validation_split=0.1)\n    \n    test_data_t=train_data_t[:100]\n    test_data_a = train_data_a[:100]\n    test_label=train_label[:100]\n#     test_mask=train_mask[:100]\n    model.fit([train_data_t[100:], train_data_a[100:]], train_label[100:],epochs=5,batch_size=20,shuffle=True)#,callbacks=[TestCallback((merged_test_data,test_label,test_mask))]) #validation_split=0.1\n#     result = model.predict(merged_test_data)\n#     calc_test_result(result, test_label)\n    \n    final = model.predict([test_data_t, test_data_a])\n    calc_test_result(final, test_label)\n    return final, test_label\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result, test_label = Bimodal()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=[]\ny_pred=[]\n\nfor i in range(result.shape[0]):\n    for j in range(result.shape[1]):\n        if np.argmax(test_label[i,j] )!=0 and np.argmax(result[i,j] )!=0:\n            y_true.append(np.argmax(test_label[i,j] ))\n            y_pred.append(np.argmax(result[i,j] ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom matplotlib import pyplot as plt\n\ndef get_report(y_true, y_pred, classes):\n    clf_report = classification_report(y_true, y_pred, labels=classes, zero_division=0)\n    clf_report = clf_report.replace('\\n\\n', '\\n')\n    clf_report = clf_report.replace('micro avg', 'micro_avg')\n    clf_report = clf_report.replace('macro avg', 'macro_avg')\n    clf_report = clf_report.replace('weighted avg', 'weighted_avg')\n    clf_report = clf_report.replace(' / ', '/')\n    lines = clf_report.split('\\n')\n\n    class_names, plotMat, support = [], [], []\n    for line in lines[1:]:\n        t = line.strip().split()\n        if len(t) < 2:\n            continue\n        v = [float(x) for x in t[1: len(t) - 1]]\n        if len(v) == 1 : v = v * 3\n        support.append(int(t[-1]))\n        class_names.append(t[0])\n        plotMat.append(v)\n    plotMat = np.array(plotMat)\n    support = np.array(support)\n    return class_names, plotMat, support\n\ndef get_scores(y_true, y_pred, classes):\n    correct, wrong = {}, {}\n    for tag in classes:\n        correct[tag] = 0\n        wrong[tag] = 0\n        \n    for tag, pred in zip(y_true, y_pred):\n        if tag == pred:\n            correct[tag] += 1\n        else:\n            wrong[tag] += 1\n            \n    scores = []\n    total = len(y_true)\n    for tag in classes:\n        cur = np.array([correct[tag], wrong[tag]])\n        scores.append(cur / total)\n    return np.array(scores)\n    \ndef plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues):\n    cm = np.copy(mat)\n    title = 'Confusion Matrix (without normalization)'\n    if normalize:\n        cm = cm.astype('float') / np.sum(cm, axis=1, keepdims=True)\n        title = title.replace('without', 'with')\n    plt.clf()    \n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.clim(vmin=0.0, vmax=1.0)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = np.max(cm) / 2\n    thresh = 1 / 2\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            color = \"white\" if (cm[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=color)\n    plt.ylabel('True label',fontsize=22)\n    plt.xlabel('Predicted label', fontsize=22)\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()\n    \ndef plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues):\n    title = 'Classification Report'\n    xticklabels = ['Precision', 'Recall', 'F1-score']\n    yticklabels = ['{0} ({1})'.format(classes[idx], sup) for idx, sup in enumerate(support)]\n    plt.clf()\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.set_tick_params(labelsize=18)\n    ax.yaxis.set_tick_params(labelsize=14)\n    plt.imshow(plotMat, interpolation='nearest', cmap=cmap, aspect='auto')\n    plt.clim(vmin=0.0, vmax=1.0)\n    plt.colorbar()\n    plt.xticks(np.arange(3), xticklabels, rotation=0)\n    plt.yticks(np.arange(len(classes)), yticklabels)\n    thresh = np.max(plotMat) / 2\n    thresh = 1 / 2\n    for i in range(plotMat.shape[0]):\n        for j in range(plotMat.shape[1]):\n            color = \"white\" if (plotMat[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(plotMat[i, j], '.2f'), horizontalalignment=\"center\", color=color, fontsize=14)\n\n    plt.xlabel('Metrics',fontsize=22)\n    plt.ylabel('Classes',fontsize=22)\n    plt.tight_layout()\n    plt.savefig('classification_report.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()\n    \ndef plot_tag_scores(classes, scores, normalize=True):\n    plt.clf()\n    width = 0.45\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.xaxis.set_tick_params(labelsize=18, rotation=25)\n    ax.yaxis.set_tick_params(labelsize=18)\n    range_bar1 = np.arange(len(classes))\n    rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color='b')\n    rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color='r')\n\n    ax.set_ylabel('Scores',fontsize=22)\n    ax.set_title('Tag scores', fontsize=22)\n    ax.set_xticks(range_bar1 + width / 2)\n    ax.set_xticklabels(classes)\n\n    ax.legend((rects1[0], rects2[0]), ('Correct', 'Wrong'), fontsize=20)\n    plt.legend()\n    plt.savefig('tag_scores.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()\n\n\nclasses=[1,2]\nclass_names, report, support = get_report(y_true, y_pred, classes)\ncm = confusion_matrix(y_true, y_pred, labels=classes)\nscores = get_scores(y_true, y_pred, classes)\nplot_clf_report(class_names, report, support)\nplot_confusion_matrix(classes, cm)\nplot_tag_scores(classes, scores)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}