<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>trimodal-fusion-for-iemocap API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>trimodal-fusion-for-iemocap</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
np.random.seed(1337)
import keras
from keras import backend as K
from keras.layers.core import Lambda
from keras.engine.topology import Layer
from keras.layers.convolutional import Conv1D
from keras.models import Sequential
from sklearn.metrics import classification_report
from matplotlib import pyplot as plt

from keras.layers.core import Flatten
from keras.callbacks import EarlyStopping

import pickle
from keras.layers.merge import Multiply,Concatenate
from sklearn.metrics import classification_report

from sklearn.metrics import accuracy_score
from keras.optimizers import Adam
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from keras.callbacks import Callback
from keras.models import Model
from keras.layers.wrappers import TimeDistributed, Bidirectional
from keras.layers import GRU,LSTM,Concatenate,Dropout,Masking,Input,Dense,Activation
from keras.layers.pooling import AveragePooling1D,MaxPooling1D
import sys
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_fscore_support
from keras.layers.core import Reshape
from keras.backend import shape
from keras.utils import plot_model







def createOneHot(train_label,  test_label):
    &#39;&#39;&#39;
    this function takes a np array of labels and converts it into a one hot encoded 2D matrix for use with categorical_crossentropy loss

    Args:
    train_label: a 1d numpy array of integers corresponding to labels
    test_label: a 1d numpy array of integers corresponding to labels

    Returns:
        a pair of 2D numpy arrays of size (dimension 0 of corresponding input, max label value in correspodning input+1)
    &#39;&#39;&#39;
    return keras.utils.to_categorical(train_label), keras.utils.to_categorical(test_label)


def report_plot(result, test_label, test_mask):
    &#39;&#39;&#39;
    Generates various classification stats in terminal

    Args:
    result: a 3D numpy array of predictions of model in one hot encoded form (test_size by sequence_length by number of classes)
    test_label: a 3D numpy array of true labels in one hot encoded form (test_size by sequence_length by number of classes)
    test_mask: a 2D numpy array telling which inputs to ignore while calculating accuracies

    Returns: None
    &#39;&#39;&#39;
    true_label=[]
    predicted_label=[]

    for i in range(result.shape[0]):
        for j in range(result.shape[1]):
            if test_mask[i,j]==1:
                true_label.append(np.argmax(test_label[i,j] ))
                predicted_label.append(np.argmax(result[i,j] ))
    
    print(&#34;Confusion Matrix :&#34;)
    print(confusion_matrix(true_label, predicted_label))
    print(&#34;Classification Report :&#34;)
    print(classification_report(true_label, predicted_label,digits=4))
    print(&#34;Accuracy &#34;, accuracy_score(true_label, predicted_label))
    print(&#34;Macro Classification Report :&#34;)
    print(precision_recall_fscore_support(true_label, predicted_label,average=&#39;macro&#39;))
    print(&#34;Weighted Classification Report :&#34;)
    print(precision_recall_fscore_support(true_label, predicted_label,average=&#39;weighted&#39;))


# In[3]:



NUM_LABELS=4


def load_bim_acts(): 
    &#39;&#39;&#39;
    loads the pickle file saved at &#34;./bim_act.pickle&#34; containing a dictionary of bimodal activations

    Returns:
    merged_train_data: 3D numpy array of input to trimodal layers
    merged_test_data: 3D numpy array to test trimodal layers
    train_label: 3D numpy array of one hot encoded labels
    test_label: 3D numpy array of one hot encoded labels to test trimodal layers 
    train_mask: 2D numpy array telling utternaces to ignore in train data
    test_mask: 2D numpy array telling utternaces to ignore in test data
    max_len: maximum sequence length
    &#39;&#39;&#39;
    with open(&#39;./bim_act.pickle&#39;, &#39;rb&#39;) as handle:
        activations = pickle.load(handle, encoding = &#39;latin1&#39;)  
    merged_train_data = activations[&#39;train_bimodal&#39;]
    merged_test_data = activations[&#39;test_bimodal&#39;]
    train_mask=activations[&#39;train_mask&#39;]
    test_mask=activations[&#39;test_mask&#39;]
    train_label=activations[&#39;train_label&#39;]
    test_label=activations[&#39;test_label&#39;]


  #Setting dummy utterances to be 0
    for i in range(activations[&#39;train_bimodal&#39;].shape[0]):
        for j in range(activations[&#39;train_bimodal&#39;].shape[1]):
            if train_mask[i][j] == 0.0 :
                merged_train_data[i,j,:]=0.0

    for i in range(activations[&#39;test_bimodal&#39;].shape[0]):
        for j in range(activations[&#39;test_bimodal&#39;].shape[1]):
            if test_mask[i][j] == 0.0 :
                merged_test_data[i,j,:]=0.0

    audio_dim,visual_dim,visual_dim=[500]*3
    dim = audio_dim + visual_dim + text_dim
    max_len = merged_train_data.shape[1] #max number of utterances per video
    dim_proj=450
    return merged_train_data, merged_test_data, train_label, test_label, train_mask, test_mask, max_len


def load_unim_acts():
    &#39;&#39;&#39;
    load unimodal activations saved at &#34;../input/multimodal-sentiment/unimodal.pickle&#34; 
    The pickle file must contain a dictionary of numpy arrays having the feature vectors with keys: 
    &#39;audio_train&#39;, &#39;video_train&#39;, &#39;audio_test&#39;, &#39;text_train&#39;, &#39;test_mask&#39;, &#39;test_label&#39;, &#39;video_test&#39;, &#39;train_mask&#39;, &#39;text_test&#39;, &#39;train_label&#39;

    Returns:
    merged_train_data: 3D numpy array of input to trimodal layers
    merged_test_data: 3D numpy array to test trimodal layers
    train_label: 3D numpy array of one hot encoded labels
    test_label: 3D numpy array of one hot encoded labels to test trimodal layers 
    train_mask: 2D numpy array telling utternaces to ignore in train data
    test_mask: 2D numpy array telling utternaces to ignore in test data
    &#39;&#39;&#39;
    with open(&#39;../input/multimodal-sentiment/unimodal.pickle&#39;, &#39;rb&#39;) as pic:
        unim_acts = pickle.load(pic, encoding = &#39;latin1&#39;)

    merged_train_data = np.concatenate((unim_acts[&#39;text_train&#39;], unim_acts[&#39;audio_train&#39;], unim_acts[&#39;video_train&#39;]), axis=2)
    merged_test_data = np.concatenate((unim_acts[&#39;text_test&#39;], unim_acts[&#39;audio_test&#39;], unim_acts[&#39;video_test&#39;]), axis=2)
  
    train_mask,test_mask,train_label,test_label=[unim_acts[&#39;train_mask&#39;],unim_acts[&#39;test_mask&#39;],unim_acts[&#39;train_label&#39;],unim_acts[&#39;test_label&#39;]]

    #Setting dummy utterances to be 0
    for i in range(unim_acts[&#39;audio_train&#39;].shape[0]):
        for j in range(unim_acts[&#39;audio_train&#39;].shape[1]):
            if train_mask[i][j] == 0.0 :
                merged_train_data[i,j,:]=0.0

    for i in range(unim_acts[&#39;audio_test&#39;].shape[0]):
        for j in range(unim_acts[&#39;audio_test&#39;].shape[1]):
            if test_mask[i][j] == 0.0 :
                merged_test_data[i,j,:]=0.0
    global audio_dim, visual_dim, text_dim, dim, max_len, dim_proj


    audio_dim,visual_dim,text_dim=[unim_acts[&#39;audio_train&#39;].shape[2],unim_acts[&#39;video_train&#39;].shape[2],unim_acts[&#39;text_train&#39;].shape[2]]
    dim = audio_dim + visual_dim + text_dim
    max_len = merged_train_data.shape[1] #max number of utterances per video
    dim_proj=450

    return merged_train_data, merged_test_data, train_label, test_label, train_mask, test_mask


# In[4]:


TRIGRU_SZ=450

def Bimodal():
    &#39;&#39;&#39;
    Trains bimodal layers using the input feature vector and creates the bim_act.pickle file. The bimodal models and fusion layers are defined herein

    Returns:
    None
    &#39;&#39;&#39;
    class bim_fusion_layer(Layer): # fusion layer for bimodal context
        &#39;&#39;&#39;
        This class is the bimodal fusion layers that carries out the fusion of pairs
        of modalities using weighted sum
        &#39;&#39;&#39;
        def __init__(self, prefix, **kwargs):
            &#39;&#39;&#39;
            Initialises the layer:
            Args:
            prefix: string that can be used to name internal weights
            kwargs: are the arguments to the superclass Layer
            &#39;&#39;&#39;
            self.supports_masking = True
            self.prefix = prefix
            super(bim_fusion_layer, self).__init__(**kwargs)

        def build(self, input_shape):
            &#39;&#39;&#39;
            Takes the input shame and initalises the traianble used for the hadamard product
            Args:
            input_shape: numpy array as required by the build in superclass Layer
            &#39;&#39;&#39;
            self.output_dim = dim_proj
            self.wt1 = self.add_weight(name=self.prefix+&#39;wt1&#39;, shape=(self.output_dim,),initializer=&#39;TruncatedNormal&#39;,trainable=True)
            self.wt2 = self.add_weight(name=self.prefix+&#39;wt2&#39;, shape=(self.output_dim,),initializer=&#39;TruncatedNormal&#39;,trainable=True)
            self.bias = self.add_weight(name=self.prefix+&#39;bias&#39;, shape=(self.output_dim,),initializer=&#39;zeros&#39;,trainable=True)
            super(bim_fusion_layer, self).build(input_shape) 

        def call(self, x):
            &#39;&#39;&#39;
            Forward propagation of the layer
            Args:
            x: list having the modalities

            Returns:
            output value of the layer 
            &#39;&#39;&#39;
            assert (K.int_shape(x[0])[1] &lt;= dim_proj)
            return K.tanh(x[0]*self.wt1 + x[1]*self.wt2 + self.bias)

        def compute_output_shape(self, input_shape):
            &#39;&#39;&#39;
            Args:
            input_shape: tuple
            Returns:
            output shape after feed forward
            &#39;&#39;&#39;
            return (input_shape[0][0],self.output_dim)



    merged_train_data, merged_test_data, train_label, test_label, train_mask, test_mask = load_unim_acts()
  

    x=Input(shape=(audio_dim+visual_dim+text_dim,), name=&#39;bi_input&#39;) # lambda layers to extract the various modalities
    masked = Masking(mask_value =0.0 , name=&#39;bi_mask&#39;)(x)
    ia = Lambda(lambda x: x[:, 0:text_dim],output_shape=lambda x:(x[0], text_dim), name=&#39;bi_crop1&#39;)(masked)
    ib =Lambda(lambda x: x[:, text_dim:(text_dim+visual_dim)],output_shape=lambda x:(x[0], visual_dim), name=&#39;bi_crop2&#39;)(masked)
    ic =Lambda(lambda x: x[:, (text_dim+visual_dim):dim],output_shape=lambda x:(x[0], audio_dim), name=&#39;bi_crop3&#39;)(masked)

    d1=Dense(dim_proj, activation=&#39;tanh&#39;, use_bias=False, trainable=True, name=&#39;bid1&#39;)(ia)
    d2=Dense(dim_proj, activation=&#39;tanh&#39;, use_bias=False, trainable=True, name=&#39;bid2&#39;)(ib)
    d3=Dense(dim_proj, activation=&#39;tanh&#39;, use_bias=False, trainable=True, name=&#39;bid3&#39;)(ic)
    
    fus12=bim_fusion_layer(&#39;ha&#39;)([d1,d2]) #fusion layers to group modalities in pairs
    fus13=bim_fusion_layer(&#39;hb&#39;)([d1,d3])
    fus23=bim_fusion_layer(&#39;hc&#39;)([d2,d3])
    bim12=Model(x,outputs=fus12)
    bim13=Model(x,outputs=fus13)
    bim23=Model(x,outputs=fus23)


    #the 3 gru that are applied on the three pairs to give bimodal fused context
    input_data = Input(shape=(max_len, K.int_shape(fus12)[1],), name=&#39;input1_gru&#39;)  
    grubi1 = GRU(600, activation=&#39;tanh&#39;, return_sequences = True, dropout=0.4, name=&#39;gru1&#39;)(input_data)
    tmp = Dropout(0.9)(grubi1)
    tmp = TimeDistributed(Dense(500,activation=&#39;tanh&#39;))(tmp)
    o1 = TimeDistributed(Dense(NUM_LABELS,activation=&#39;softmax&#39;))(tmp)
    gru1 = Model(input_data, o1)
    aux1 = Model(input_data, tmp)
    input_data = Input(shape=(max_len, K.int_shape(fus12)[1],), name=&#39;input2_gru&#39;)
    grubi2 = GRU(600, activation=&#39;tanh&#39;, return_sequences = True, dropout=0.4, name=&#39;gru2&#39;)(input_data)
    tmp = Dropout(0.9)(grubi2)
    tmp = TimeDistributed(Dense(500,activation=&#39;tanh&#39;))(tmp)
    o2 = TimeDistributed(Dense(NUM_LABELS,activation=&#39;softmax&#39;))(tmp)
    gru2 = Model(input_data, o2)
    aux2 = Model(input_data, tmp)
    input_data = Input(shape=(max_len, K.int_shape(fus12)[1],), name=&#39;input3_gru&#39;)
    grubi3 = GRU(600, activation=&#39;tanh&#39;, return_sequences = True, dropout=0.4, name=&#39;gru3&#39;)(input_data)
    tmp = Dropout(0.9)(grubi3)
    tmp = TimeDistributed(Dense(500,activation=&#39;tanh&#39;))(tmp)
    o3 = TimeDistributed(Dense(NUM_LABELS,activation=&#39;softmax&#39;))(tmp)
    gru3 = Model(input_data, o3)
    aux3 = Model(input_data, tmp)


    
    main_input=Input(shape=(max_len,audio_dim+visual_dim+text_dim,), name=&#39;bimod_input&#39;)
    
    #final classification layer into num_labels used to train bimodal layer
    uttr12=TimeDistributed(bim12, name=&#39;bi_tdis_b12&#39;)(main_input)
    uttr13=TimeDistributed(bim13, name=&#39;bi_tdis_b13&#39;)(main_input)
    uttr23=TimeDistributed(bim23, name=&#39;bi_tdis_b23&#39;)(main_input)

   #separating output of GRU before dense layer to get bimodal representation
    auxi12=aux1(uttr12)
    auxi13=aux2(uttr13)
    auxi23=aux3(uttr23)

    #defining the 6 models
    context_1_2=gru1(uttr12)
    BiModal1 = Model(main_input, context_1_2)
    Predictor1 = Model(main_input, auxi12)
    context_1_3=gru2(uttr13)
    BiModal2 = Model(main_input, context_1_3)
    Predictor2 = Model(main_input, auxi13)
    context_2_3=gru3(uttr23)
    BiModal3 = Model(main_input, context_2_3)
    Predictor3 = Model(main_input, auxi23)


    #training and taking output from axillary layers to serve as input to trimodal layers
    optimizer=Adam(lr=0.001)
    BiModal1.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, sample_weight_mode=&#39;temporal&#39;)
    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10)
    BiModal1.fit(merged_train_data, train_label,epochs=200,batch_size=20,sample_weight=train_mask,shuffle=True, callbacks=[early_stopping],validation_split=0.1)
    train_result1 = Predictor1.predict(merged_train_data)
    test_result1 = Predictor1.predict(merged_test_data)
    BiModal2.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, sample_weight_mode=&#39;temporal&#39;)
    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10)
    BiModal2.fit(merged_train_data, train_label,epochs=200,batch_size=20,sample_weight=train_mask,shuffle=True, callbacks=[early_stopping],validation_split=0.1)
    train_result2 = Predictor2.predict(merged_train_data)
    test_result2 = Predictor2.predict(merged_test_data)
    BiModal3.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, sample_weight_mode=&#39;temporal&#39;)
    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10)
    BiModal3.fit(merged_train_data, train_label,epochs=200,batch_size=20,sample_weight=train_mask,shuffle=True, callbacks=[early_stopping],validation_split=0.1)
    train_result3 = Predictor3.predict(merged_train_data)
    test_result3 = Predictor3.predict(merged_test_data)
    train_bim = np.concatenate((train_result1, train_result2, train_result3), axis=2)
    test_bim = np.concatenate((test_result1, test_result2, test_result3), axis=2)

    #saving output
    bim_dict={}
    bim_dict[&#39;train_bimodal&#39;] = train_bim
    bim_dict[&#39;test_bimodal&#39;] = test_bim
    bim_dict[&#39;train_mask&#39;]=train_mask
    bim_dict[&#39;test_mask&#39;]= test_mask
    bim_dict[&#39;train_label&#39;]=train_label
    bim_dict[&#39;test_label&#39;]=test_label

    #merge all output
    with open(&#39;bim_act.pickle&#39;, &#39;wb&#39;) as handle:
        pickle.dump(bim_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print(&#34;bimodal acts saved&#34;)


def Trimodal():
    &#39;&#39;&#39;
    Trains the trimodal layers
    Returns:
    result: a 3D numpy array of predictions of model in one hot encoded form (test_size by sequence_length by number of classes)
    test_label: a 3D numpy array of true labels in one hot encoded form (test_size by sequence_length by number of classes)
    test_mask: a 2D numpy array telling which inputs to ignore while calculating accuracies
    &#39;&#39;&#39;

    merged_train_data, merged_test_data, train_label, test_label, train_mask, test_mask,maxlen = load_bim_acts()


    class trifusion_layer(Layer): #trimodal fusion layer: weighted sum followed by tanh
        &#39;&#39;&#39;
        trimodal fusion layer: weighted sum followed by tanh
        &#39;&#39;&#39;
        def __init__(self, **kwargs):
            &#39;&#39;&#39;
            Initialises the layer:
            Args:
            prefix: string that can be used to name internal weights
            kwargs: are the arguments to the superclass Layer
            &#39;&#39;&#39;
            self.supports_masking = True
            super(trifusion_layer, self).__init__(**kwargs)

        def build(self, input_shape):
            &#39;&#39;&#39;
            Takes the input shame and initalises the traianble used for the hadamard product
            Args:
            input_shape: numpy array as required by the build in superclass Layer
            &#39;&#39;&#39;
            self.output_dim = 500

            self.wt1 = self.add_weight(name=&#39;wt1&#39;, shape=(self.output_dim,),initializer=&#39;glorot_uniform&#39;,trainable=True)
            self.wt2 = self.add_weight(name=&#39;wt2&#39;, shape=(self.output_dim,),initializer=&#39;glorot_uniform&#39;,trainable=True)
            self.wt3 = self.add_weight(name=&#39;wt3&#39;, shape=(self.output_dim,),initializer=&#39;glorot_uniform&#39;,trainable=True)
            self.bias = self.add_weight(name=&#39;bias&#39;, shape=(self.output_dim,),initializer=&#39;zeros&#39;,trainable=True)
            super(trifusion_layer, self).build(input_shape) 

        def call(self, x, mask=None):
            &#39;&#39;&#39;
            Forward propagation of the layer
            Args:
            x: list having the modalities

            Returns:
            output value of the layer 
            &#39;&#39;&#39;
            x1=Lambda(lambda x: x[:,0:500],output_shape=lambda x: (x[0],500))(x)
            x2=Lambda(lambda x: x[:,500:1000],output_shape=lambda x: (x[0],500))(x)
            x3=Lambda(lambda x: x[:,1000:1500],output_shape=lambda x: (x[0],500))(x)
            ai = K.tanh(x1*self.wt1 + x2*self.wt2 + x3*self.wt3 + self.bias)
            return ai

        def compute_output_shape(self, x):
            &#39;&#39;&#39;
            Args:
            input_shape: tuple
            Returns:
            output shape after feed forward
            &#39;&#39;&#39;
            return (x[0],self.output_dim)
        def compute_mask(self, input, input_mask=None):
            &#39;&#39;&#39;
            since masking is being done by mask layer we return list of None here
            Args:
            input to model and mask
            Returns:
            list of Nones
            &#39;&#39;&#39;
            if isinstance(input_mask, list):
                return [None] * len(input_mask)
            else:
                return None

    #defining input to the model and the trimodal layer to be applied separately at all time steps
    x_three=Input(shape=(1500,), name=&#39;tri_input&#39;)
    allfused=trifusion_layer()(x_three)
    hfusion_model=Model(x_three,outputs=allfused)

    #defining the final trimodal architecture to classify into NUM_LABELS using the trimodal fused representations
    main_input= Input(shape=(maxlen,1500,), name=&#39;tri_main_input&#39;)
    mask_vals = Masking(mask_value =0.0)(main_input)
    vidutt=TimeDistributed(hfusion_model)(mask_vals)
    grutri = GRU(TRIGRU_SZ, activation=&#39;tanh&#39;, return_sequences = True, dropout=0.35, name=&#39;tri_lstm&#39;)(vidutt)
    grutri = Masking(mask_value=0.0)(grutri)
    tmp = Dropout(0.86)(grutri)
    concat=Concatenate(name=&#34;final_concatenation&#34;)([vidutt,tmp])
    tmp2 = TimeDistributed(Dense(450,activation=&#39;relu&#39;))(concat)
    tmp2 = Masking(mask_value=0.)(tmp2)
    cffull = Dropout(0.86)(tmp2)
    cffull = Masking(mask_value=0.)(cffull)
    preds = Dense(NUM_LABELS, activation=&#39;softmax&#39;)(cffull)



    #training the model
    model=Model(main_input,outputs=preds)

    optimizer=Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, sample_weight_mode=&#39;temporal&#39;,metrics=[&#39;accuracy&#39;])

    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=5)
    
    model.fit(merged_train_data, train_label,epochs=16,batch_size=20,sample_weight=train_mask,shuffle=True,validation_split=0.1)
    result = model.predict(merged_test_data)
    report_plot(result, test_label, test_mask)  
    return result, test_label, test_mask


# In[5]:



    

def get_report(y_true, y_pred, classes):
    &#39;&#39;&#39;This function parses the classification report given by sklearn to
    get all the row names metric values as floats and supports for each
    class label.
    Args:
        y_true: true (numerical) labels of data
        y_pred: predicted (numerical) labels of the same data
        classes: a python list of class labels
    Returns:
        class_names: a python list of class labels (here, row names from report)
        plotMat: numerical values (metrics) in the classification report
        support: the number of instances for each class_name present in report
    &#39;&#39;&#39;
    clf_report = classification_report(y_true, y_pred, labels=classes, zero_division=0)
    clf_report = clf_report.replace(&#39;\n\n&#39;, &#39;\n&#39;)
    clf_report = clf_report.replace(&#39;micro avg&#39;, &#39;micro_avg&#39;)
    clf_report = clf_report.replace(&#39;macro avg&#39;, &#39;macro_avg&#39;)
    clf_report = clf_report.replace(&#39;weighted avg&#39;, &#39;weighted_avg&#39;)
    clf_report = clf_report.replace(&#39; / &#39;, &#39;/&#39;)
    lines = clf_report.split(&#39;\n&#39;)

    class_names, plotMat, support = [], [], []
    for line in lines[1:]:
        t = line.strip().split()
        if len(t) &lt; 2:
            continue
        v = [float(x) for x in t[1: len(t) - 1]]
        if len(v) == 1 : v = v * 3
        support.append(int(t[-1]))
        class_names.append(t[0])
        plotMat.append(v)
    plotMat = np.array(plotMat)
    support = np.array(support)
    return class_names, plotMat, support

def get_scores(y_true, y_pred, classes):
    &#39;&#39;&#39;This function calculates the correct and incorrect counts for each label
    as a fraction to the total instances of that class.
    Args:
        y_true: true (numerical) labels of data
        y_pred: predicted (numerical) labels of the same data
        classes: a python list of class labels
    Returns:
        numpy array of tuple of (correct,incorrect) fractions for each class
    &#39;&#39;&#39;
    correct, wrong = {}, {}
    for tag in classes:
        correct[tag] = 0
        wrong[tag] = 0

    for tag, pred in zip(y_true, y_pred):
        if tag == pred:
            correct[tag] += 1
        else:
            wrong[tag] += 1

    scores = []
    total = len(y_true)
    for tag in classes:
        cur = np.array([correct[tag], wrong[tag]])
        scores.append(cur / total)
    return np.array(scores)

def plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues):
    &#34;&#34;&#34;This function plots the confusion matrix as an image, using the 
    parsed values from the confusion matrix and saves the image in 
    the current working directory.
    Args:
        classes: a python list of class labels
        mat: numerical values (metrics) in the confusion matrix
        normalize: controls the normalization of the confusion 
            matrix (rows sum to 1 or not)
        cmap: the color map to be used in the output image
        filename: the filename with which the plot will be saved (can be a path too)
    Returns:
        No return value. Shows and saves the confusion matrix image.
    &#34;&#34;&#34;
    cm = np.copy(mat)
    title = &#39;Confusion Matrix (without normalization)&#39;
    if normalize:
        cm = cm.astype(&#39;float&#39;) / np.sum(cm, axis=1, keepdims=True)
        title = title.replace(&#39;without&#39;, &#39;with&#39;)
    plt.clf()    
    fig, ax = plt.subplots(figsize=(20,10))
    ax.set_title(title, y=-0.06, fontsize=22)
    ax.xaxis.set_ticks_position(&#39;top&#39;)
    ax.xaxis.set_label_position(&#39;top&#39;)
    plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap)
    plt.clim(vmin=0.0, vmax=1.0)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    fmt = &#39;.2f&#39; if normalize else &#39;d&#39;
    thresh = np.max(cm) / 2
    thresh = 1 / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            color = &#34;white&#34; if (cm[i, j] &gt; thresh) else &#34;black&#34;
            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&#34;center&#34;, color=color)
    plt.ylabel(&#39;True label&#39;,fontsize=22)
    plt.xlabel(&#39;Predicted label&#39;, fontsize=22)
    plt.tight_layout()
    plt.savefig(&#39;confusion_matrix.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
    plt.show()

def plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues):
    &#34;&#34;&#34;This function plots the classification report as an image, using the 
    parsed values from the sklearn classification report and saves the image in 
    the current working directory.
    Args:
        classes: a python list of class labels
        plotMat: numerical values (metrics) in the classification report
        support: the number of instances for each class present in report
        cmap: the color map to be used in the output image
        filename: the filename with which the plot will be saved (can be a path too)
    Returns:
        No return value. Shows and saves the report image.
    &#34;&#34;&#34;
    title = &#39;Classification Report&#39;
    xticklabels = [&#39;Precision&#39;, &#39;Recall&#39;, &#39;F1-score&#39;]
    yticklabels = [&#39;{0} ({1})&#39;.format(classes[idx], sup) for idx, sup in enumerate(support)]
    plt.clf()
    fig, ax = plt.subplots(figsize=(20,10))
    ax.set_title(title, y=-0.06, fontsize=22)
    ax.xaxis.set_ticks_position(&#39;top&#39;)
    ax.xaxis.set_label_position(&#39;top&#39;)
    ax.xaxis.set_tick_params(labelsize=18)
    ax.yaxis.set_tick_params(labelsize=14)
    plt.imshow(plotMat, interpolation=&#39;nearest&#39;, cmap=cmap, aspect=&#39;auto&#39;)
    plt.clim(vmin=0.0, vmax=1.0)
    plt.colorbar()
    plt.xticks(np.arange(3), xticklabels, rotation=0)
    plt.yticks(np.arange(len(classes)), yticklabels)
    thresh = np.max(plotMat) / 2
    thresh = 1 / 2
    for i in range(plotMat.shape[0]):
        for j in range(plotMat.shape[1]):
            color = &#34;white&#34; if (plotMat[i, j] &gt; thresh) else &#34;black&#34;
            plt.text(j, i, format(plotMat[i, j], &#39;.2f&#39;), horizontalalignment=&#34;center&#34;, color=color, fontsize=14)

    plt.xlabel(&#39;Metrics&#39;,fontsize=22)
    plt.ylabel(&#39;Classes&#39;,fontsize=22)
    plt.tight_layout()
    plt.savefig(&#39;classification_report.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
    plt.show()

def plot_tag_scores(classes, scores, normalize=True):
    &#34;&#34;&#34;This function plots the histogram for tag scores and saves the image in 
    the current working directory.
    Args:
        classes: a python list of class labels
        scores: a dictionary of correct and incorrect counts for each label
        filename: the filename with which the plot will be saved (can be a path too)
    Returns:
        No return value. Shows and saves the tag scores plot.
    &#34;&#34;&#34;
    plt.clf()
    width = 0.45
    fig, ax = plt.subplots(figsize=(20,10))
    ax.xaxis.set_tick_params(labelsize=18, rotation=25)
    ax.yaxis.set_tick_params(labelsize=18)
    range_bar1 = np.arange(len(classes))
    rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color=&#39;b&#39;)
    rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color=&#39;r&#39;)

    ax.set_ylabel(&#39;Scores&#39;,fontsize=22)
    ax.set_title(&#39;Tag scores&#39;, fontsize=22)
    ax.set_xticks(range_bar1 + width / 2)
    ax.set_xticklabels(classes)

    ax.legend((rects1[0], rects2[0]), (&#39;Correct&#39;, &#39;Wrong&#39;), fontsize=20)
    plt.legend()
    plt.savefig(&#39;tag_scores.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
    plt.show()


# In[6]:


if __name__==&#39;__main__&#39;:
    Bimodal()
    result, test_label, test_mask=Trimodal()
    y_true=[]
    y_pred=[]
    for i in range(result.shape[0]):
        for j in range(result.shape[1]):
            if test_mask[i,j]==1:
                y_true.append(np.argmax(test_label[i,j] ))
                y_pred.append(np.argmax(result[i,j] ))

    classes=[0,1,2,3]
    class_names, report, support = get_report(y_true, y_pred, classes)
    cm = confusion_matrix(y_true, y_pred, labels=classes)
    scores = get_scores(y_true, y_pred, classes)
    plot_clf_report(class_names, report, support)
    plot_confusion_matrix(classes, cm)
    plot_tag_scores(classes, scores)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="trimodal-fusion-for-iemocap.Bimodal"><code class="name flex">
<span>def <span class="ident">Bimodal</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains bimodal layers using the input feature vector and creates the bim_act.pickle file. The bimodal models and fusion layers are defined herein</p>
<p>Returns:
None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Bimodal():
    &#39;&#39;&#39;
    Trains bimodal layers using the input feature vector and creates the bim_act.pickle file. The bimodal models and fusion layers are defined herein

    Returns:
    None
    &#39;&#39;&#39;
    class bim_fusion_layer(Layer): # fusion layer for bimodal context
        &#39;&#39;&#39;
        This class is the bimodal fusion layers that carries out the fusion of pairs
        of modalities using weighted sum
        &#39;&#39;&#39;
        def __init__(self, prefix, **kwargs):
            &#39;&#39;&#39;
            Initialises the layer:
            Args:
            prefix: string that can be used to name internal weights
            kwargs: are the arguments to the superclass Layer
            &#39;&#39;&#39;
            self.supports_masking = True
            self.prefix = prefix
            super(bim_fusion_layer, self).__init__(**kwargs)

        def build(self, input_shape):
            &#39;&#39;&#39;
            Takes the input shame and initalises the traianble used for the hadamard product
            Args:
            input_shape: numpy array as required by the build in superclass Layer
            &#39;&#39;&#39;
            self.output_dim = dim_proj
            self.wt1 = self.add_weight(name=self.prefix+&#39;wt1&#39;, shape=(self.output_dim,),initializer=&#39;TruncatedNormal&#39;,trainable=True)
            self.wt2 = self.add_weight(name=self.prefix+&#39;wt2&#39;, shape=(self.output_dim,),initializer=&#39;TruncatedNormal&#39;,trainable=True)
            self.bias = self.add_weight(name=self.prefix+&#39;bias&#39;, shape=(self.output_dim,),initializer=&#39;zeros&#39;,trainable=True)
            super(bim_fusion_layer, self).build(input_shape) 

        def call(self, x):
            &#39;&#39;&#39;
            Forward propagation of the layer
            Args:
            x: list having the modalities

            Returns:
            output value of the layer 
            &#39;&#39;&#39;
            assert (K.int_shape(x[0])[1] &lt;= dim_proj)
            return K.tanh(x[0]*self.wt1 + x[1]*self.wt2 + self.bias)

        def compute_output_shape(self, input_shape):
            &#39;&#39;&#39;
            Args:
            input_shape: tuple
            Returns:
            output shape after feed forward
            &#39;&#39;&#39;
            return (input_shape[0][0],self.output_dim)



    merged_train_data, merged_test_data, train_label, test_label, train_mask, test_mask = load_unim_acts()
  

    x=Input(shape=(audio_dim+visual_dim+text_dim,), name=&#39;bi_input&#39;) # lambda layers to extract the various modalities
    masked = Masking(mask_value =0.0 , name=&#39;bi_mask&#39;)(x)
    ia = Lambda(lambda x: x[:, 0:text_dim],output_shape=lambda x:(x[0], text_dim), name=&#39;bi_crop1&#39;)(masked)
    ib =Lambda(lambda x: x[:, text_dim:(text_dim+visual_dim)],output_shape=lambda x:(x[0], visual_dim), name=&#39;bi_crop2&#39;)(masked)
    ic =Lambda(lambda x: x[:, (text_dim+visual_dim):dim],output_shape=lambda x:(x[0], audio_dim), name=&#39;bi_crop3&#39;)(masked)

    d1=Dense(dim_proj, activation=&#39;tanh&#39;, use_bias=False, trainable=True, name=&#39;bid1&#39;)(ia)
    d2=Dense(dim_proj, activation=&#39;tanh&#39;, use_bias=False, trainable=True, name=&#39;bid2&#39;)(ib)
    d3=Dense(dim_proj, activation=&#39;tanh&#39;, use_bias=False, trainable=True, name=&#39;bid3&#39;)(ic)
    
    fus12=bim_fusion_layer(&#39;ha&#39;)([d1,d2]) #fusion layers to group modalities in pairs
    fus13=bim_fusion_layer(&#39;hb&#39;)([d1,d3])
    fus23=bim_fusion_layer(&#39;hc&#39;)([d2,d3])
    bim12=Model(x,outputs=fus12)
    bim13=Model(x,outputs=fus13)
    bim23=Model(x,outputs=fus23)


    #the 3 gru that are applied on the three pairs to give bimodal fused context
    input_data = Input(shape=(max_len, K.int_shape(fus12)[1],), name=&#39;input1_gru&#39;)  
    grubi1 = GRU(600, activation=&#39;tanh&#39;, return_sequences = True, dropout=0.4, name=&#39;gru1&#39;)(input_data)
    tmp = Dropout(0.9)(grubi1)
    tmp = TimeDistributed(Dense(500,activation=&#39;tanh&#39;))(tmp)
    o1 = TimeDistributed(Dense(NUM_LABELS,activation=&#39;softmax&#39;))(tmp)
    gru1 = Model(input_data, o1)
    aux1 = Model(input_data, tmp)
    input_data = Input(shape=(max_len, K.int_shape(fus12)[1],), name=&#39;input2_gru&#39;)
    grubi2 = GRU(600, activation=&#39;tanh&#39;, return_sequences = True, dropout=0.4, name=&#39;gru2&#39;)(input_data)
    tmp = Dropout(0.9)(grubi2)
    tmp = TimeDistributed(Dense(500,activation=&#39;tanh&#39;))(tmp)
    o2 = TimeDistributed(Dense(NUM_LABELS,activation=&#39;softmax&#39;))(tmp)
    gru2 = Model(input_data, o2)
    aux2 = Model(input_data, tmp)
    input_data = Input(shape=(max_len, K.int_shape(fus12)[1],), name=&#39;input3_gru&#39;)
    grubi3 = GRU(600, activation=&#39;tanh&#39;, return_sequences = True, dropout=0.4, name=&#39;gru3&#39;)(input_data)
    tmp = Dropout(0.9)(grubi3)
    tmp = TimeDistributed(Dense(500,activation=&#39;tanh&#39;))(tmp)
    o3 = TimeDistributed(Dense(NUM_LABELS,activation=&#39;softmax&#39;))(tmp)
    gru3 = Model(input_data, o3)
    aux3 = Model(input_data, tmp)


    
    main_input=Input(shape=(max_len,audio_dim+visual_dim+text_dim,), name=&#39;bimod_input&#39;)
    
    #final classification layer into num_labels used to train bimodal layer
    uttr12=TimeDistributed(bim12, name=&#39;bi_tdis_b12&#39;)(main_input)
    uttr13=TimeDistributed(bim13, name=&#39;bi_tdis_b13&#39;)(main_input)
    uttr23=TimeDistributed(bim23, name=&#39;bi_tdis_b23&#39;)(main_input)

   #separating output of GRU before dense layer to get bimodal representation
    auxi12=aux1(uttr12)
    auxi13=aux2(uttr13)
    auxi23=aux3(uttr23)

    #defining the 6 models
    context_1_2=gru1(uttr12)
    BiModal1 = Model(main_input, context_1_2)
    Predictor1 = Model(main_input, auxi12)
    context_1_3=gru2(uttr13)
    BiModal2 = Model(main_input, context_1_3)
    Predictor2 = Model(main_input, auxi13)
    context_2_3=gru3(uttr23)
    BiModal3 = Model(main_input, context_2_3)
    Predictor3 = Model(main_input, auxi23)


    #training and taking output from axillary layers to serve as input to trimodal layers
    optimizer=Adam(lr=0.001)
    BiModal1.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, sample_weight_mode=&#39;temporal&#39;)
    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10)
    BiModal1.fit(merged_train_data, train_label,epochs=200,batch_size=20,sample_weight=train_mask,shuffle=True, callbacks=[early_stopping],validation_split=0.1)
    train_result1 = Predictor1.predict(merged_train_data)
    test_result1 = Predictor1.predict(merged_test_data)
    BiModal2.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, sample_weight_mode=&#39;temporal&#39;)
    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10)
    BiModal2.fit(merged_train_data, train_label,epochs=200,batch_size=20,sample_weight=train_mask,shuffle=True, callbacks=[early_stopping],validation_split=0.1)
    train_result2 = Predictor2.predict(merged_train_data)
    test_result2 = Predictor2.predict(merged_test_data)
    BiModal3.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, sample_weight_mode=&#39;temporal&#39;)
    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10)
    BiModal3.fit(merged_train_data, train_label,epochs=200,batch_size=20,sample_weight=train_mask,shuffle=True, callbacks=[early_stopping],validation_split=0.1)
    train_result3 = Predictor3.predict(merged_train_data)
    test_result3 = Predictor3.predict(merged_test_data)
    train_bim = np.concatenate((train_result1, train_result2, train_result3), axis=2)
    test_bim = np.concatenate((test_result1, test_result2, test_result3), axis=2)

    #saving output
    bim_dict={}
    bim_dict[&#39;train_bimodal&#39;] = train_bim
    bim_dict[&#39;test_bimodal&#39;] = test_bim
    bim_dict[&#39;train_mask&#39;]=train_mask
    bim_dict[&#39;test_mask&#39;]= test_mask
    bim_dict[&#39;train_label&#39;]=train_label
    bim_dict[&#39;test_label&#39;]=test_label

    #merge all output
    with open(&#39;bim_act.pickle&#39;, &#39;wb&#39;) as handle:
        pickle.dump(bim_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print(&#34;bimodal acts saved&#34;)</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.Trimodal"><code class="name flex">
<span>def <span class="ident">Trimodal</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the trimodal layers
Returns:
result: a 3D numpy array of predictions of model in one hot encoded form (test_size by sequence_length by number of classes)
test_label: a 3D numpy array of true labels in one hot encoded form (test_size by sequence_length by number of classes)
test_mask: a 2D numpy array telling which inputs to ignore while calculating accuracies</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Trimodal():
    &#39;&#39;&#39;
    Trains the trimodal layers
    Returns:
    result: a 3D numpy array of predictions of model in one hot encoded form (test_size by sequence_length by number of classes)
    test_label: a 3D numpy array of true labels in one hot encoded form (test_size by sequence_length by number of classes)
    test_mask: a 2D numpy array telling which inputs to ignore while calculating accuracies
    &#39;&#39;&#39;

    merged_train_data, merged_test_data, train_label, test_label, train_mask, test_mask,maxlen = load_bim_acts()


    class trifusion_layer(Layer): #trimodal fusion layer: weighted sum followed by tanh
        &#39;&#39;&#39;
        trimodal fusion layer: weighted sum followed by tanh
        &#39;&#39;&#39;
        def __init__(self, **kwargs):
            &#39;&#39;&#39;
            Initialises the layer:
            Args:
            prefix: string that can be used to name internal weights
            kwargs: are the arguments to the superclass Layer
            &#39;&#39;&#39;
            self.supports_masking = True
            super(trifusion_layer, self).__init__(**kwargs)

        def build(self, input_shape):
            &#39;&#39;&#39;
            Takes the input shame and initalises the traianble used for the hadamard product
            Args:
            input_shape: numpy array as required by the build in superclass Layer
            &#39;&#39;&#39;
            self.output_dim = 500

            self.wt1 = self.add_weight(name=&#39;wt1&#39;, shape=(self.output_dim,),initializer=&#39;glorot_uniform&#39;,trainable=True)
            self.wt2 = self.add_weight(name=&#39;wt2&#39;, shape=(self.output_dim,),initializer=&#39;glorot_uniform&#39;,trainable=True)
            self.wt3 = self.add_weight(name=&#39;wt3&#39;, shape=(self.output_dim,),initializer=&#39;glorot_uniform&#39;,trainable=True)
            self.bias = self.add_weight(name=&#39;bias&#39;, shape=(self.output_dim,),initializer=&#39;zeros&#39;,trainable=True)
            super(trifusion_layer, self).build(input_shape) 

        def call(self, x, mask=None):
            &#39;&#39;&#39;
            Forward propagation of the layer
            Args:
            x: list having the modalities

            Returns:
            output value of the layer 
            &#39;&#39;&#39;
            x1=Lambda(lambda x: x[:,0:500],output_shape=lambda x: (x[0],500))(x)
            x2=Lambda(lambda x: x[:,500:1000],output_shape=lambda x: (x[0],500))(x)
            x3=Lambda(lambda x: x[:,1000:1500],output_shape=lambda x: (x[0],500))(x)
            ai = K.tanh(x1*self.wt1 + x2*self.wt2 + x3*self.wt3 + self.bias)
            return ai

        def compute_output_shape(self, x):
            &#39;&#39;&#39;
            Args:
            input_shape: tuple
            Returns:
            output shape after feed forward
            &#39;&#39;&#39;
            return (x[0],self.output_dim)
        def compute_mask(self, input, input_mask=None):
            &#39;&#39;&#39;
            since masking is being done by mask layer we return list of None here
            Args:
            input to model and mask
            Returns:
            list of Nones
            &#39;&#39;&#39;
            if isinstance(input_mask, list):
                return [None] * len(input_mask)
            else:
                return None

    #defining input to the model and the trimodal layer to be applied separately at all time steps
    x_three=Input(shape=(1500,), name=&#39;tri_input&#39;)
    allfused=trifusion_layer()(x_three)
    hfusion_model=Model(x_three,outputs=allfused)

    #defining the final trimodal architecture to classify into NUM_LABELS using the trimodal fused representations
    main_input= Input(shape=(maxlen,1500,), name=&#39;tri_main_input&#39;)
    mask_vals = Masking(mask_value =0.0)(main_input)
    vidutt=TimeDistributed(hfusion_model)(mask_vals)
    grutri = GRU(TRIGRU_SZ, activation=&#39;tanh&#39;, return_sequences = True, dropout=0.35, name=&#39;tri_lstm&#39;)(vidutt)
    grutri = Masking(mask_value=0.0)(grutri)
    tmp = Dropout(0.86)(grutri)
    concat=Concatenate(name=&#34;final_concatenation&#34;)([vidutt,tmp])
    tmp2 = TimeDistributed(Dense(450,activation=&#39;relu&#39;))(concat)
    tmp2 = Masking(mask_value=0.)(tmp2)
    cffull = Dropout(0.86)(tmp2)
    cffull = Masking(mask_value=0.)(cffull)
    preds = Dense(NUM_LABELS, activation=&#39;softmax&#39;)(cffull)



    #training the model
    model=Model(main_input,outputs=preds)

    optimizer=Adam(lr=0.0001)
    model.compile(optimizer=optimizer, loss=&#39;categorical_crossentropy&#39;, sample_weight_mode=&#39;temporal&#39;,metrics=[&#39;accuracy&#39;])

    early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=5)
    
    model.fit(merged_train_data, train_label,epochs=16,batch_size=20,sample_weight=train_mask,shuffle=True,validation_split=0.1)
    result = model.predict(merged_test_data)
    report_plot(result, test_label, test_mask)  
    return result, test_label, test_mask</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.createOneHot"><code class="name flex">
<span>def <span class="ident">createOneHot</span></span>(<span>train_label, test_label)</span>
</code></dt>
<dd>
<div class="desc"><p>this function takes a np array of labels and converts it into a one hot encoded 2D matrix for use with categorical_crossentropy loss</p>
<p>Args:
train_label: a 1d numpy array of integers corresponding to labels
test_label: a 1d numpy array of integers corresponding to labels</p>
<h2 id="returns">Returns</h2>
<p>a pair of 2D numpy arrays of size (dimension 0 of corresponding input, max label value in correspodning input+1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def createOneHot(train_label,  test_label):
    &#39;&#39;&#39;
    this function takes a np array of labels and converts it into a one hot encoded 2D matrix for use with categorical_crossentropy loss

    Args:
    train_label: a 1d numpy array of integers corresponding to labels
    test_label: a 1d numpy array of integers corresponding to labels

    Returns:
        a pair of 2D numpy arrays of size (dimension 0 of corresponding input, max label value in correspodning input+1)
    &#39;&#39;&#39;
    return keras.utils.to_categorical(train_label), keras.utils.to_categorical(test_label)</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.get_report"><code class="name flex">
<span>def <span class="ident">get_report</span></span>(<span>y_true, y_pred, classes)</span>
</code></dt>
<dd>
<div class="desc"><p>This function parses the classification report given by sklearn to
get all the row names metric values as floats and supports for each
class label.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y_true</code></strong></dt>
<dd>true (numerical) labels of data</dd>
<dt><strong><code>y_pred</code></strong></dt>
<dd>predicted (numerical) labels of the same data</dd>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>class_names</code></dt>
<dd>a python list of class labels (here, row names from report)</dd>
<dt><code>plotMat</code></dt>
<dd>numerical values (metrics) in the classification report</dd>
<dt><code>support</code></dt>
<dd>the number of instances for each class_name present in report</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_report(y_true, y_pred, classes):
    &#39;&#39;&#39;This function parses the classification report given by sklearn to
    get all the row names metric values as floats and supports for each
    class label.
    Args:
        y_true: true (numerical) labels of data
        y_pred: predicted (numerical) labels of the same data
        classes: a python list of class labels
    Returns:
        class_names: a python list of class labels (here, row names from report)
        plotMat: numerical values (metrics) in the classification report
        support: the number of instances for each class_name present in report
    &#39;&#39;&#39;
    clf_report = classification_report(y_true, y_pred, labels=classes, zero_division=0)
    clf_report = clf_report.replace(&#39;\n\n&#39;, &#39;\n&#39;)
    clf_report = clf_report.replace(&#39;micro avg&#39;, &#39;micro_avg&#39;)
    clf_report = clf_report.replace(&#39;macro avg&#39;, &#39;macro_avg&#39;)
    clf_report = clf_report.replace(&#39;weighted avg&#39;, &#39;weighted_avg&#39;)
    clf_report = clf_report.replace(&#39; / &#39;, &#39;/&#39;)
    lines = clf_report.split(&#39;\n&#39;)

    class_names, plotMat, support = [], [], []
    for line in lines[1:]:
        t = line.strip().split()
        if len(t) &lt; 2:
            continue
        v = [float(x) for x in t[1: len(t) - 1]]
        if len(v) == 1 : v = v * 3
        support.append(int(t[-1]))
        class_names.append(t[0])
        plotMat.append(v)
    plotMat = np.array(plotMat)
    support = np.array(support)
    return class_names, plotMat, support</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.get_scores"><code class="name flex">
<span>def <span class="ident">get_scores</span></span>(<span>y_true, y_pred, classes)</span>
</code></dt>
<dd>
<div class="desc"><p>This function calculates the correct and incorrect counts for each label
as a fraction to the total instances of that class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y_true</code></strong></dt>
<dd>true (numerical) labels of data</dd>
<dt><strong><code>y_pred</code></strong></dt>
<dd>predicted (numerical) labels of the same data</dd>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>numpy array of tuple of (correct,incorrect) fractions for each class</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_scores(y_true, y_pred, classes):
    &#39;&#39;&#39;This function calculates the correct and incorrect counts for each label
    as a fraction to the total instances of that class.
    Args:
        y_true: true (numerical) labels of data
        y_pred: predicted (numerical) labels of the same data
        classes: a python list of class labels
    Returns:
        numpy array of tuple of (correct,incorrect) fractions for each class
    &#39;&#39;&#39;
    correct, wrong = {}, {}
    for tag in classes:
        correct[tag] = 0
        wrong[tag] = 0

    for tag, pred in zip(y_true, y_pred):
        if tag == pred:
            correct[tag] += 1
        else:
            wrong[tag] += 1

    scores = []
    total = len(y_true)
    for tag in classes:
        cur = np.array([correct[tag], wrong[tag]])
        scores.append(cur / total)
    return np.array(scores)</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.load_bim_acts"><code class="name flex">
<span>def <span class="ident">load_bim_acts</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>loads the pickle file saved at "./bim_act.pickle" containing a dictionary of bimodal activations</p>
<p>Returns:
merged_train_data: 3D numpy array of input to trimodal layers
merged_test_data: 3D numpy array to test trimodal layers
train_label: 3D numpy array of one hot encoded labels
test_label: 3D numpy array of one hot encoded labels to test trimodal layers
train_mask: 2D numpy array telling utternaces to ignore in train data
test_mask: 2D numpy array telling utternaces to ignore in test data
max_len: maximum sequence length</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_bim_acts(): 
    &#39;&#39;&#39;
    loads the pickle file saved at &#34;./bim_act.pickle&#34; containing a dictionary of bimodal activations

    Returns:
    merged_train_data: 3D numpy array of input to trimodal layers
    merged_test_data: 3D numpy array to test trimodal layers
    train_label: 3D numpy array of one hot encoded labels
    test_label: 3D numpy array of one hot encoded labels to test trimodal layers 
    train_mask: 2D numpy array telling utternaces to ignore in train data
    test_mask: 2D numpy array telling utternaces to ignore in test data
    max_len: maximum sequence length
    &#39;&#39;&#39;
    with open(&#39;./bim_act.pickle&#39;, &#39;rb&#39;) as handle:
        activations = pickle.load(handle, encoding = &#39;latin1&#39;)  
    merged_train_data = activations[&#39;train_bimodal&#39;]
    merged_test_data = activations[&#39;test_bimodal&#39;]
    train_mask=activations[&#39;train_mask&#39;]
    test_mask=activations[&#39;test_mask&#39;]
    train_label=activations[&#39;train_label&#39;]
    test_label=activations[&#39;test_label&#39;]


  #Setting dummy utterances to be 0
    for i in range(activations[&#39;train_bimodal&#39;].shape[0]):
        for j in range(activations[&#39;train_bimodal&#39;].shape[1]):
            if train_mask[i][j] == 0.0 :
                merged_train_data[i,j,:]=0.0

    for i in range(activations[&#39;test_bimodal&#39;].shape[0]):
        for j in range(activations[&#39;test_bimodal&#39;].shape[1]):
            if test_mask[i][j] == 0.0 :
                merged_test_data[i,j,:]=0.0

    audio_dim,visual_dim,visual_dim=[500]*3
    dim = audio_dim + visual_dim + text_dim
    max_len = merged_train_data.shape[1] #max number of utterances per video
    dim_proj=450
    return merged_train_data, merged_test_data, train_label, test_label, train_mask, test_mask, max_len</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.load_unim_acts"><code class="name flex">
<span>def <span class="ident">load_unim_acts</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>load unimodal activations saved at "../input/multimodal-sentiment/unimodal.pickle"
The pickle file must contain a dictionary of numpy arrays having the feature vectors with keys:
'audio_train', 'video_train', 'audio_test', 'text_train', 'test_mask', 'test_label', 'video_test', 'train_mask', 'text_test', 'train_label'</p>
<p>Returns:
merged_train_data: 3D numpy array of input to trimodal layers
merged_test_data: 3D numpy array to test trimodal layers
train_label: 3D numpy array of one hot encoded labels
test_label: 3D numpy array of one hot encoded labels to test trimodal layers
train_mask: 2D numpy array telling utternaces to ignore in train data
test_mask: 2D numpy array telling utternaces to ignore in test data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_unim_acts():
    &#39;&#39;&#39;
    load unimodal activations saved at &#34;../input/multimodal-sentiment/unimodal.pickle&#34; 
    The pickle file must contain a dictionary of numpy arrays having the feature vectors with keys: 
    &#39;audio_train&#39;, &#39;video_train&#39;, &#39;audio_test&#39;, &#39;text_train&#39;, &#39;test_mask&#39;, &#39;test_label&#39;, &#39;video_test&#39;, &#39;train_mask&#39;, &#39;text_test&#39;, &#39;train_label&#39;

    Returns:
    merged_train_data: 3D numpy array of input to trimodal layers
    merged_test_data: 3D numpy array to test trimodal layers
    train_label: 3D numpy array of one hot encoded labels
    test_label: 3D numpy array of one hot encoded labels to test trimodal layers 
    train_mask: 2D numpy array telling utternaces to ignore in train data
    test_mask: 2D numpy array telling utternaces to ignore in test data
    &#39;&#39;&#39;
    with open(&#39;../input/multimodal-sentiment/unimodal.pickle&#39;, &#39;rb&#39;) as pic:
        unim_acts = pickle.load(pic, encoding = &#39;latin1&#39;)

    merged_train_data = np.concatenate((unim_acts[&#39;text_train&#39;], unim_acts[&#39;audio_train&#39;], unim_acts[&#39;video_train&#39;]), axis=2)
    merged_test_data = np.concatenate((unim_acts[&#39;text_test&#39;], unim_acts[&#39;audio_test&#39;], unim_acts[&#39;video_test&#39;]), axis=2)
  
    train_mask,test_mask,train_label,test_label=[unim_acts[&#39;train_mask&#39;],unim_acts[&#39;test_mask&#39;],unim_acts[&#39;train_label&#39;],unim_acts[&#39;test_label&#39;]]

    #Setting dummy utterances to be 0
    for i in range(unim_acts[&#39;audio_train&#39;].shape[0]):
        for j in range(unim_acts[&#39;audio_train&#39;].shape[1]):
            if train_mask[i][j] == 0.0 :
                merged_train_data[i,j,:]=0.0

    for i in range(unim_acts[&#39;audio_test&#39;].shape[0]):
        for j in range(unim_acts[&#39;audio_test&#39;].shape[1]):
            if test_mask[i][j] == 0.0 :
                merged_test_data[i,j,:]=0.0
    global audio_dim, visual_dim, text_dim, dim, max_len, dim_proj


    audio_dim,visual_dim,text_dim=[unim_acts[&#39;audio_train&#39;].shape[2],unim_acts[&#39;video_train&#39;].shape[2],unim_acts[&#39;text_train&#39;].shape[2]]
    dim = audio_dim + visual_dim + text_dim
    max_len = merged_train_data.shape[1] #max number of utterances per video
    dim_proj=450

    return merged_train_data, merged_test_data, train_label, test_label, train_mask, test_mask</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.plot_clf_report"><code class="name flex">
<span>def <span class="ident">plot_clf_report</span></span>(<span>classes, plotMat, support, cmap=&lt;matplotlib.colors.LinearSegmentedColormap object&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>This function plots the classification report as an image, using the
parsed values from the sklearn classification report and saves the image in
the current working directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
<dt><strong><code>plotMat</code></strong></dt>
<dd>numerical values (metrics) in the classification report</dd>
<dt><strong><code>support</code></strong></dt>
<dd>the number of instances for each class present in report</dd>
<dt><strong><code>cmap</code></strong></dt>
<dd>the color map to be used in the output image</dd>
<dt><strong><code>filename</code></strong></dt>
<dd>the filename with which the plot will be saved (can be a path too)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>No return value. Shows and saves the report image.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues):
    &#34;&#34;&#34;This function plots the classification report as an image, using the 
    parsed values from the sklearn classification report and saves the image in 
    the current working directory.
    Args:
        classes: a python list of class labels
        plotMat: numerical values (metrics) in the classification report
        support: the number of instances for each class present in report
        cmap: the color map to be used in the output image
        filename: the filename with which the plot will be saved (can be a path too)
    Returns:
        No return value. Shows and saves the report image.
    &#34;&#34;&#34;
    title = &#39;Classification Report&#39;
    xticklabels = [&#39;Precision&#39;, &#39;Recall&#39;, &#39;F1-score&#39;]
    yticklabels = [&#39;{0} ({1})&#39;.format(classes[idx], sup) for idx, sup in enumerate(support)]
    plt.clf()
    fig, ax = plt.subplots(figsize=(20,10))
    ax.set_title(title, y=-0.06, fontsize=22)
    ax.xaxis.set_ticks_position(&#39;top&#39;)
    ax.xaxis.set_label_position(&#39;top&#39;)
    ax.xaxis.set_tick_params(labelsize=18)
    ax.yaxis.set_tick_params(labelsize=14)
    plt.imshow(plotMat, interpolation=&#39;nearest&#39;, cmap=cmap, aspect=&#39;auto&#39;)
    plt.clim(vmin=0.0, vmax=1.0)
    plt.colorbar()
    plt.xticks(np.arange(3), xticklabels, rotation=0)
    plt.yticks(np.arange(len(classes)), yticklabels)
    thresh = np.max(plotMat) / 2
    thresh = 1 / 2
    for i in range(plotMat.shape[0]):
        for j in range(plotMat.shape[1]):
            color = &#34;white&#34; if (plotMat[i, j] &gt; thresh) else &#34;black&#34;
            plt.text(j, i, format(plotMat[i, j], &#39;.2f&#39;), horizontalalignment=&#34;center&#34;, color=color, fontsize=14)

    plt.xlabel(&#39;Metrics&#39;,fontsize=22)
    plt.ylabel(&#39;Classes&#39;,fontsize=22)
    plt.tight_layout()
    plt.savefig(&#39;classification_report.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
    plt.show()</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.plot_confusion_matrix"><code class="name flex">
<span>def <span class="ident">plot_confusion_matrix</span></span>(<span>classes, mat, normalize=True, cmap=&lt;matplotlib.colors.LinearSegmentedColormap object&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>This function plots the confusion matrix as an image, using the
parsed values from the confusion matrix and saves the image in
the current working directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
<dt><strong><code>mat</code></strong></dt>
<dd>numerical values (metrics) in the confusion matrix</dd>
<dt><strong><code>normalize</code></strong></dt>
<dd>controls the normalization of the confusion
matrix (rows sum to 1 or not)</dd>
<dt><strong><code>cmap</code></strong></dt>
<dd>the color map to be used in the output image</dd>
<dt><strong><code>filename</code></strong></dt>
<dd>the filename with which the plot will be saved (can be a path too)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>No return value. Shows and saves the confusion matrix image.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues):
    &#34;&#34;&#34;This function plots the confusion matrix as an image, using the 
    parsed values from the confusion matrix and saves the image in 
    the current working directory.
    Args:
        classes: a python list of class labels
        mat: numerical values (metrics) in the confusion matrix
        normalize: controls the normalization of the confusion 
            matrix (rows sum to 1 or not)
        cmap: the color map to be used in the output image
        filename: the filename with which the plot will be saved (can be a path too)
    Returns:
        No return value. Shows and saves the confusion matrix image.
    &#34;&#34;&#34;
    cm = np.copy(mat)
    title = &#39;Confusion Matrix (without normalization)&#39;
    if normalize:
        cm = cm.astype(&#39;float&#39;) / np.sum(cm, axis=1, keepdims=True)
        title = title.replace(&#39;without&#39;, &#39;with&#39;)
    plt.clf()    
    fig, ax = plt.subplots(figsize=(20,10))
    ax.set_title(title, y=-0.06, fontsize=22)
    ax.xaxis.set_ticks_position(&#39;top&#39;)
    ax.xaxis.set_label_position(&#39;top&#39;)
    plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap)
    plt.clim(vmin=0.0, vmax=1.0)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    fmt = &#39;.2f&#39; if normalize else &#39;d&#39;
    thresh = np.max(cm) / 2
    thresh = 1 / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            color = &#34;white&#34; if (cm[i, j] &gt; thresh) else &#34;black&#34;
            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&#34;center&#34;, color=color)
    plt.ylabel(&#39;True label&#39;,fontsize=22)
    plt.xlabel(&#39;Predicted label&#39;, fontsize=22)
    plt.tight_layout()
    plt.savefig(&#39;confusion_matrix.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
    plt.show()</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.plot_tag_scores"><code class="name flex">
<span>def <span class="ident">plot_tag_scores</span></span>(<span>classes, scores, normalize=True)</span>
</code></dt>
<dd>
<div class="desc"><p>This function plots the histogram for tag scores and saves the image in
the current working directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
<dt><strong><code>scores</code></strong></dt>
<dd>a dictionary of correct and incorrect counts for each label</dd>
<dt><strong><code>filename</code></strong></dt>
<dd>the filename with which the plot will be saved (can be a path too)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>No return value. Shows and saves the tag scores plot.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_tag_scores(classes, scores, normalize=True):
    &#34;&#34;&#34;This function plots the histogram for tag scores and saves the image in 
    the current working directory.
    Args:
        classes: a python list of class labels
        scores: a dictionary of correct and incorrect counts for each label
        filename: the filename with which the plot will be saved (can be a path too)
    Returns:
        No return value. Shows and saves the tag scores plot.
    &#34;&#34;&#34;
    plt.clf()
    width = 0.45
    fig, ax = plt.subplots(figsize=(20,10))
    ax.xaxis.set_tick_params(labelsize=18, rotation=25)
    ax.yaxis.set_tick_params(labelsize=18)
    range_bar1 = np.arange(len(classes))
    rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color=&#39;b&#39;)
    rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color=&#39;r&#39;)

    ax.set_ylabel(&#39;Scores&#39;,fontsize=22)
    ax.set_title(&#39;Tag scores&#39;, fontsize=22)
    ax.set_xticks(range_bar1 + width / 2)
    ax.set_xticklabels(classes)

    ax.legend((rects1[0], rects2[0]), (&#39;Correct&#39;, &#39;Wrong&#39;), fontsize=20)
    plt.legend()
    plt.savefig(&#39;tag_scores.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
    plt.show()</code></pre>
</details>
</dd>
<dt id="trimodal-fusion-for-iemocap.report_plot"><code class="name flex">
<span>def <span class="ident">report_plot</span></span>(<span>result, test_label, test_mask)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates various classification stats in terminal</p>
<p>Args:
result: a 3D numpy array of predictions of model in one hot encoded form (test_size by sequence_length by number of classes)
test_label: a 3D numpy array of true labels in one hot encoded form (test_size by sequence_length by number of classes)
test_mask: a 2D numpy array telling which inputs to ignore while calculating accuracies</p>
<p>Returns: None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def report_plot(result, test_label, test_mask):
    &#39;&#39;&#39;
    Generates various classification stats in terminal

    Args:
    result: a 3D numpy array of predictions of model in one hot encoded form (test_size by sequence_length by number of classes)
    test_label: a 3D numpy array of true labels in one hot encoded form (test_size by sequence_length by number of classes)
    test_mask: a 2D numpy array telling which inputs to ignore while calculating accuracies

    Returns: None
    &#39;&#39;&#39;
    true_label=[]
    predicted_label=[]

    for i in range(result.shape[0]):
        for j in range(result.shape[1]):
            if test_mask[i,j]==1:
                true_label.append(np.argmax(test_label[i,j] ))
                predicted_label.append(np.argmax(result[i,j] ))
    
    print(&#34;Confusion Matrix :&#34;)
    print(confusion_matrix(true_label, predicted_label))
    print(&#34;Classification Report :&#34;)
    print(classification_report(true_label, predicted_label,digits=4))
    print(&#34;Accuracy &#34;, accuracy_score(true_label, predicted_label))
    print(&#34;Macro Classification Report :&#34;)
    print(precision_recall_fscore_support(true_label, predicted_label,average=&#39;macro&#39;))
    print(&#34;Weighted Classification Report :&#34;)
    print(precision_recall_fscore_support(true_label, predicted_label,average=&#39;weighted&#39;))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="trimodal-fusion-for-iemocap.Bimodal" href="#trimodal-fusion-for-iemocap.Bimodal">Bimodal</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.Trimodal" href="#trimodal-fusion-for-iemocap.Trimodal">Trimodal</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.createOneHot" href="#trimodal-fusion-for-iemocap.createOneHot">createOneHot</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.get_report" href="#trimodal-fusion-for-iemocap.get_report">get_report</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.get_scores" href="#trimodal-fusion-for-iemocap.get_scores">get_scores</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.load_bim_acts" href="#trimodal-fusion-for-iemocap.load_bim_acts">load_bim_acts</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.load_unim_acts" href="#trimodal-fusion-for-iemocap.load_unim_acts">load_unim_acts</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.plot_clf_report" href="#trimodal-fusion-for-iemocap.plot_clf_report">plot_clf_report</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.plot_confusion_matrix" href="#trimodal-fusion-for-iemocap.plot_confusion_matrix">plot_confusion_matrix</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.plot_tag_scores" href="#trimodal-fusion-for-iemocap.plot_tag_scores">plot_tag_scores</a></code></li>
<li><code><a title="trimodal-fusion-for-iemocap.report_plot" href="#trimodal-fusion-for-iemocap.report_plot">report_plot</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>