{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(337) # for reproducibility\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.layers.core import Lambda\nfrom keras.engine.topology import Layer\nfrom keras.models import Sequential\nfrom keras.layers import Input,Dense,GRU,LSTM,Concatenate,Dropout,Activation,Add, Masking\nfrom keras.layers.pooling import AveragePooling1D,MaxPooling1D\nfrom keras.layers.core import Flatten\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers.convolutional import Conv1D\nfrom keras.models import Model\nfrom keras.layers.wrappers import TimeDistributed, Bidirectional\nfrom keras.layers.core import Reshape\nfrom keras.backend import shape\nfrom keras.utils import plot_model\nfrom keras.layers.merge import Multiply,Concatenate\nimport pickle\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom keras.optimizers import RMSprop,Adadelta,Adam\nfrom keras.callbacks import Callback\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_test_result(result, test_label, test_mask):\n    true_label=[]\n    predicted_label=[]\n\n    for i in range(result.shape[0]):\n        for j in range(result.shape[1]):\n            if test_mask[i][j]==1:\n                true_label.append(np.argmax(test_label[i,j] ))\n                predicted_label.append(np.argmax(result[i,j] ))\n    \n    print(\"Confusion Matrix :\")\n    print(confusion_matrix(true_label, predicted_label))\n    print(\"Classification Report :\")\n    print(classification_report(true_label, predicted_label,digits=4))\n    print(\"Accuracy \", accuracy_score(true_label, predicted_label))\n    print(\"Macro Classification Report :\")\n    print(precision_recall_fscore_support(true_label, predicted_label,average='macro'))\n    print(\"Weighted Classification Report :\")\n    print(precision_recall_fscore_support(true_label, predicted_label,average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_LABELS=4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def Unimodal_text():\n    with open('../input/multimodal-sentiment/unimodal.pickle', 'rb') as handle:\n        unimodal_activations = pickle.load(handle, encoding = 'latin1')\n        \n    train_data = unimodal_activations['text_train']\n    train_label = unimodal_activations['train_label']\n    test_data = unimodal_activations['text_test']\n    test_label = unimodal_activations['test_label']\n    dim_1 = train_data.shape[0]\n    max_len = train_data.shape[1]\n    dim_text = train_data.shape[2]\n    \n    dimt_1 = test_data.shape[0]\n    maxt_len = test_data.shape[1]\n    \n    test_mask=unimodal_activations['test_mask']\n    train_mask=unimodal_activations['train_mask']\n    \n    for i in range(dim_1):\n        for j in range(max_len):\n            if train_mask[i][j] == 0.0 :\n                train_data[i,j,:]=0.0\n\n    for i in range(dimt_1):\n        for j in range(maxt_len):\n            if test_mask[i][j] == 0.0 :\n                test_data[i,j,:]=0.0\n    \n    print(train_data.shape)\n    print(train_label.shape)\n    \n    model = Sequential()\n    \n    model.add(Input(shape=(max_len, dim_text,), name='Input_unimodal'))\n    model.add(GRU(500, return_sequences=True, dropout=0.2, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3)))\n    model.add(TimeDistributed(Dense(NUM_LABELS,activation='softmax',kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3))))\n    \n    model.compile(loss='categorical_crossentropy',optimizer=Adam(0.0001),metrics=['accuracy'])\n    print(model.summary())\n    model.fit(train_data, train_label, epochs=10, batch_size=20, validation_split=0.1)\n    \n    final = model.predict(test_data)\n    calc_test_result(final, test_label, test_mask)\n    \n    return final, test_label, test_mask\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result, test_label, test_mask = Unimodal_text()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=[]\ny_pred=[]\n\nfor i in range(result.shape[0]):\n    for j in range(result.shape[1]):\n        if test_mask[i][j]==1:\n            y_true.append(np.argmax(test_label[i,j] ))\n            y_pred.append(np.argmax(result[i,j] ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom matplotlib import pyplot as plt\n\ndef get_report(y_true, y_pred, classes):\n    clf_report = classification_report(y_true, y_pred, labels=classes, zero_division=0)\n    clf_report = clf_report.replace('\\n\\n', '\\n')\n    clf_report = clf_report.replace('micro avg', 'micro_avg')\n    clf_report = clf_report.replace('macro avg', 'macro_avg')\n    clf_report = clf_report.replace('weighted avg', 'weighted_avg')\n    clf_report = clf_report.replace(' / ', '/')\n    lines = clf_report.split('\\n')\n\n    class_names, plotMat, support = [], [], []\n    for line in lines[1:]:\n        t = line.strip().split()\n        if len(t) < 2:\n            continue\n        v = [float(x) for x in t[1: len(t) - 1]]\n        if len(v) == 1 : v = v * 3\n        support.append(int(t[-1]))\n        class_names.append(t[0])\n        plotMat.append(v)\n    plotMat = np.array(plotMat)\n    support = np.array(support)\n    return class_names, plotMat, support\n\ndef get_scores(y_true, y_pred, classes):\n    correct, wrong = {}, {}\n    for tag in classes:\n        correct[tag] = 0\n        wrong[tag] = 0\n        \n    for tag, pred in zip(y_true, y_pred):\n        if tag == pred:\n            correct[tag] += 1\n        else:\n            wrong[tag] += 1\n            \n    scores = []\n    total = len(y_true)\n    for tag in classes:\n        cur = np.array([correct[tag], wrong[tag]])\n        scores.append(cur / total)\n    return np.array(scores)\n    \ndef plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues):\n    cm = np.copy(mat)\n    title = 'Confusion Matrix (without normalization)'\n    if normalize:\n        cm = cm.astype('float') / np.sum(cm, axis=1, keepdims=True)\n        title = title.replace('without', 'with')\n    plt.clf()    \n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.clim(vmin=0.0, vmax=1.0)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = np.max(cm) / 2\n    thresh = 1 / 2\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            color = \"white\" if (cm[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=color)\n    plt.ylabel('True label',fontsize=22)\n    plt.xlabel('Predicted label', fontsize=22)\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()\n    \ndef plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues):\n    title = 'Classification Report'\n    xticklabels = ['Precision', 'Recall', 'F1-score']\n    yticklabels = ['{0} ({1})'.format(classes[idx], sup) for idx, sup in enumerate(support)]\n    plt.clf()\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.set_tick_params(labelsize=18)\n    ax.yaxis.set_tick_params(labelsize=14)\n    plt.imshow(plotMat, interpolation='nearest', cmap=cmap, aspect='auto')\n    plt.clim(vmin=0.0, vmax=1.0)\n    plt.colorbar()\n    plt.xticks(np.arange(3), xticklabels, rotation=0)\n    plt.yticks(np.arange(len(classes)), yticklabels)\n    thresh = np.max(plotMat) / 2\n    thresh = 1 / 2\n    for i in range(plotMat.shape[0]):\n        for j in range(plotMat.shape[1]):\n            color = \"white\" if (plotMat[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(plotMat[i, j], '.2f'), horizontalalignment=\"center\", color=color, fontsize=14)\n\n    plt.xlabel('Metrics',fontsize=22)\n    plt.ylabel('Classes',fontsize=22)\n    plt.tight_layout()\n    plt.savefig('classification_report.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()\n    \ndef plot_tag_scores(classes, scores, normalize=True):\n    plt.clf()\n    width = 0.45\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.xaxis.set_tick_params(labelsize=18, rotation=25)\n    ax.yaxis.set_tick_params(labelsize=18)\n    range_bar1 = np.arange(len(classes))\n    rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color='b')\n    rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color='r')\n\n    ax.set_ylabel('Scores',fontsize=22)\n    ax.set_title('Tag scores', fontsize=22)\n    ax.set_xticks(range_bar1 + width / 2)\n    ax.set_xticklabels(classes)\n\n    ax.legend((rects1[0], rects2[0]), ('Correct', 'Wrong'), fontsize=20)\n    plt.legend()\n    plt.savefig('tag_scores.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()\n\n\nclasses=[0,1,2,3]\nclass_names, report, support = get_report(y_true, y_pred, classes)\ncm = confusion_matrix(y_true, y_pred, labels=classes)\nscores = get_scores(y_true, y_pred, classes)\nplot_clf_report(class_names, report, support)\nplot_confusion_matrix(classes, cm)\nplot_tag_scores(classes, scores)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}