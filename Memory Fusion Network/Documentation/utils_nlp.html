<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>utils_nlp API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>utils_nlp</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score
import matplotlib.pyplot as plt

def re_label(y):
        &#34;&#34;&#34;re_label function takes in input a list/array of any hashable
        data type, for example, strings, integers etc, which represent class labels.
        This function replaces all the class labels with integers 0 to K-1 where
        K is the number of classes (distinct elements present in input).
        Args: 
                y: python list/numpy array with each element a hashable data type
        Returns: 
                relabeled numpy array in integer format
        &#34;&#34;&#34;
        labels = sorted(list(set(list(y))))
        inv = {v:i for i,v in enumerate(labels)}
        return np.array([inv[v] for v in y], dtype=np.int32)

def remove_labels(X, y, labels_to_remove):
        &#34;&#34;&#34;remove_labels does exactly what the name suggests. This function removes
        instances from both X and y where that instance has a label which is provided
        in the argument labels_to_remove. Instances refer to rows in the data X and 
        labels y.
        Args:
                X: data having same number of instances as y
                y: numpy array of labels corresponding to the feature matrix X
                        labels_to_remove: a list of labels which will be removed from the data
                        as well as label list.
        Returns:
                X_new: X with some removed rows
                y_new: y with some removed rows
        &#34;&#34;&#34;
        indices = []
        for i in range(y.shape[0]):
                if y[i] in labels_to_remove:
                        indices.append(i)
        if isinstance(X, list):
                X_new = [np.delete(X[i], indices, axis=0) for i in range(len(X))]
        else:
                X_new = np.delete(X, indices, axis=0)
        y_new = np.delete(y, indices, axis=0)
        return X_new, y_new

def to_categorical(y, num_classes=None, dtype=&#39;float32&#39;):
        &#34;&#34;&#34;to_categorical converts labels to their one-hot encoded representation.
        Args:
                y: array/list of labels within range 0 to K-1 where K &lt;= num_classes
                num_classes: the number of classes to encoded, if None, max(y)+1 is used
                dtype: dtype of the output array
        Returns:
                one-hot encoded vector of shape NxK where N is number of examples and K is 
                number of classes.
        &#34;&#34;&#34;
        y = np.array(y, dtype=&#39;int&#39;)
        input_shape = y.shape
        if input_shape and input_shape[-1] == 1 and len(input_shape) &gt; 1:
                input_shape = tuple(input_shape[:-1])
        y = y.ravel()
        if not num_classes:
                num_classes = np.max(y) + 1
        n = y.shape[0]
        categorical = np.zeros((n, num_classes), dtype=dtype)
        categorical[np.arange(n), y] = 1
        output_shape = input_shape + (num_classes,)
        categorical = np.reshape(categorical, output_shape)
        return categorical

def round_label(a, limit=1):
        &#34;&#34;&#34;round_label rounds off labels where the label list actually is a continuous
        real value depicting the intensity of the sentiment. 
        Negative values are rounded off to nearest smaller integer
        Positive values are rounded off to nearest larger integer 
        The output is constrained to be in the range [-limit,limit]
        Args:
                a: real number (treated as intensity)
                limit: argument for constraining the output
        Returns:
                rounded off value
        &#34;&#34;&#34;
        if int(a) == a:
                res = int(a)
        elif a &gt; 0:
                res = int(a) + 1
        else:
                res = int(a) - 1
        res = max(res, -limit)
        res = min(res, limit)
        return res

def calc_test_result(output, actual):
        &#34;&#34;&#34;This function prints the confusion matrix (normalised i.e. rows sum to 1) 
        and the classification report using sklearn module.
        Args:
                output: numpy array representing class probabilities
                actual: actual label (may be one hot encoded or class probabilities)
        Returns:
                No return value, prints confusion matrix and report to stdout
        &#34;&#34;&#34;
        y_true = np.argmax(actual, axis=1)
        y_pred = np.argmax(output, axis=1)
        print(&#34;Confusion Matrix :&#34;)
        cm = confusion_matrix(y_true, y_pred)
        cm = cm.astype(&#39;float&#39;) / np.sum(cm, axis=1, keepdims=True)
        print(np.around(cm, decimals=2))
        print(&#34;Classification Report :&#34;)
        report = classification_report(y_true, y_pred, digits=4, zero_division=0)
        report = report.replace(&#39;\n\n&#39;, &#39;\n&#39;)
        print(report, end=&#39;&#39;)
        
def get_report(y_true, y_pred, classes):
        &#34;&#34;&#34;This function parses the classification report given by sklearn to
        get all the row names metric values as floats and supports for each
        class label.
        Args:
                y_true: true (numerical) labels of data
                y_pred: predicted (numerical) labels of the same data
                classes: a python list of class labels
        Returns:
                class_names: a python list of class labels (here, row names from report)
                plotMat: numerical values (metrics) in the classification report
                support: the number of instances for each class_name present in report
        &#34;&#34;&#34;
        clf_report = classification_report(y_true, y_pred, digits=4, labels=classes, zero_division=0)
        clf_report = clf_report.replace(&#39;\n\n&#39;, &#39;\n&#39;)
        clf_report = clf_report.replace(&#39;micro avg&#39;, &#39;micro_avg&#39;)
        clf_report = clf_report.replace(&#39;macro avg&#39;, &#39;macro_avg&#39;)
        clf_report = clf_report.replace(&#39;weighted avg&#39;, &#39;weighted_avg&#39;)
        clf_report = clf_report.replace(&#39; / &#39;, &#39;/&#39;)
        lines = clf_report.split(&#39;\n&#39;)

        class_names, plotMat, support = [], [], []
        for line in lines[1:]:
                t = line.strip().split()
                if len(t) &lt; 2:
                        continue
                v = [float(x) for x in t[1: len(t) - 1]]
                if len(v) == 1 : v = v * 3
                support.append(int(t[-1]))
                class_names.append(t[0])
                plotMat.append(v)
        plotMat = np.array(plotMat)
        support = np.array(support)
        return class_names, plotMat, support

def get_scores(y_true, y_pred, classes):
        &#34;&#34;&#34;This function calculates the correct and incorrect counts for each label
        as a fraction to the total instances of that class.
        Args:
                y_true: true (numerical) labels of data
                y_pred: predicted (numerical) labels of the same data
                classes: a python list of class labels
        Returns:
                numpy array of tuple of (correct,incorrect) fractions for each class
        &#34;&#34;&#34;
        correct, wrong = {}, {}
        for tag in classes:
                correct[tag] = 0
                wrong[tag] = 0
                
        for tag, pred in zip(y_true, y_pred):
                if tag == pred:
                        correct[tag] += 1
                else:
                        wrong[tag] += 1
                        
        scores = []
        total = len(y_true)
        for tag in classes:
                cur = np.array([correct[tag], wrong[tag]])
                scores.append(cur / total)
        return np.array(scores)
        
def plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues, filename=&#39;confusion_matrix&#39;):
        &#34;&#34;&#34;This function plots the confusion matrix as an image, using the 
        parsed values from the confusion matrix and saves the image in 
        the current working directory.
        Args:
                classes: a python list of class labels
                mat: numerical values (metrics) in the confusion matrix
                normalize: controls the normalization of the confusion 
                        matrix (rows sum to 1 or not)
                cmap: the color map to be used in the output image
                filename: the filename with which the plot will be saved (can be a path too)
        Returns:
                No return value. Shows and saves the confusion matrix image.
        &#34;&#34;&#34;
        k = len(classes)
        cm = np.copy(mat)
        title = &#39;Confusion Matrix (without normalization)&#39;
        if normalize:
                cm = cm.astype(&#39;float&#39;) / np.sum(cm, axis=1, keepdims=True)
                title = title.replace(&#39;without&#39;, &#39;with&#39;)
        plt.clf()    
        fig, ax = plt.subplots(figsize=(20,10))
        ax.set_title(title, y=-0.06, fontsize=66/k)
        ax.xaxis.set_ticks_position(&#39;top&#39;)
        ax.xaxis.set_label_position(&#39;top&#39;)
        plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap)
        plt.clim(vmin=0.0, vmax=1.0)
        cbar = plt.colorbar()
        cbar.ax.tick_params(labelsize=40/k) 
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.yticks(tick_marks, classes)
        fmt = &#39;.2f&#39; if normalize else &#39;d&#39;
        thresh = np.max(cm) / 2
        thresh = 1 / 2
        for i in range(cm.shape[0]):
                for j in range(cm.shape[1]):
                        color = &#34;white&#34; if (cm[i, j] &gt; thresh) else &#34;black&#34;
                        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&#34;center&#34;, color=color, fontsize=50/k)
        plt.ylabel(&#39;True label&#39;,fontsize=22)
        plt.xlabel(&#39;Predicted label&#39;, fontsize=22)
        plt.tight_layout()
        plt.savefig(filename+&#39;.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
        
def plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues, filename=&#39;classification_report&#39;):
        &#34;&#34;&#34;This function plots the classification report as an image, using the 
        parsed values from the sklearn classification report and saves the image in 
        the current working directory.
        Args:
                classes: a python list of class labels
                plotMat: numerical values (metrics) in the classification report
                support: the number of instances for each class present in report
                cmap: the color map to be used in the output image
                filename: the filename with which the plot will be saved (can be a path too)
        Returns:
                No return value. Shows and saves the report image.
        &#34;&#34;&#34;
        k = len(classes)
        title = &#39;Classification Report&#39;
        xticklabels = [&#39;Precision&#39;, &#39;Recall&#39;, &#39;F1-score&#39;]
        yticklabels = [&#39;{0} ({1})&#39;.format(classes[idx], sup) for idx, sup in enumerate(support)]
        plt.clf()
        fig, ax = plt.subplots(figsize=(22,12))
        ax.set_title(title, y=-0.06, fontsize=22)
        ax.xaxis.set_ticks_position(&#39;top&#39;)
        ax.xaxis.set_label_position(&#39;top&#39;)
        ax.xaxis.set_tick_params(labelsize=18)
        ax.yaxis.set_tick_params(labelsize=14)
        plt.imshow(plotMat, interpolation=&#39;nearest&#39;, cmap=cmap, aspect=&#39;auto&#39;)
        plt.clim(vmin=0.0, vmax=1.0)
        cbar = plt.colorbar()
        cbar.ax.tick_params(labelsize=80/k) 
        plt.xticks(np.arange(3), xticklabels, rotation=0)
        plt.yticks(np.arange(len(classes)), yticklabels)
        thresh = np.max(plotMat) / 2
        thresh = 1 / 2
        for i in range(plotMat.shape[0]):
                for j in range(plotMat.shape[1]):
                        color = &#34;white&#34; if (plotMat[i, j] &gt; thresh) else &#34;black&#34;
                        plt.text(j, i, format(plotMat[i, j], &#39;.2f&#39;), horizontalalignment=&#34;center&#34;, color=color, fontsize=100/k)

        plt.xlabel(&#39;Metrics&#39;,fontsize=22)
        plt.ylabel(&#39;Classes&#39;,fontsize=22)
        plt.tight_layout()
        plt.savefig(filename+&#39;.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
        
def plot_tag_scores(classes, scores, filename=&#39;tag_scores&#39;):
        &#34;&#34;&#34;This function plots the histogram for tag scores and saves the image in 
        the current working directory.
        Args:
                classes: a python list of class labels
                scores: a dictionary of correct and incorrect counts for each label
                filename: the filename with which the plot will be saved (can be a path too)
        Returns:
                No return value. Shows and saves the tag scores plot.
        &#34;&#34;&#34;
        plt.clf()
        width = 0.45
        fig, ax = plt.subplots(figsize=(20,10))
        ax.xaxis.set_tick_params(labelsize=18, rotation=25)
        ax.yaxis.set_tick_params(labelsize=18)
        range_bar1 = np.arange(len(classes))
        rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color=&#39;b&#39;)
        rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color=&#39;r&#39;)

        ax.set_ylabel(&#39;Scores&#39;,fontsize=22)
        ax.set_title(&#39;Tag scores&#39;, fontsize=22)
        ax.set_xticks(range_bar1 + width / 2)
        ax.set_xticklabels(classes)

        ax.legend((rects1[0], rects2[0]), (&#39;Correct&#39;, &#39;Wrong&#39;), fontsize=20)
        plt.legend()
        plt.savefig(filename+&#39;.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
        plt.show()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="utils_nlp.calc_test_result"><code class="name flex">
<span>def <span class="ident">calc_test_result</span></span>(<span>output, actual)</span>
</code></dt>
<dd>
<div class="desc"><p>This function prints the confusion matrix (normalised i.e. rows sum to 1)
and the classification report using sklearn module.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output</code></strong></dt>
<dd>numpy array representing class probabilities</dd>
<dt><strong><code>actual</code></strong></dt>
<dd>actual label (may be one hot encoded or class probabilities)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>No return value, prints confusion matrix and report to stdout</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_test_result(output, actual):
        &#34;&#34;&#34;This function prints the confusion matrix (normalised i.e. rows sum to 1) 
        and the classification report using sklearn module.
        Args:
                output: numpy array representing class probabilities
                actual: actual label (may be one hot encoded or class probabilities)
        Returns:
                No return value, prints confusion matrix and report to stdout
        &#34;&#34;&#34;
        y_true = np.argmax(actual, axis=1)
        y_pred = np.argmax(output, axis=1)
        print(&#34;Confusion Matrix :&#34;)
        cm = confusion_matrix(y_true, y_pred)
        cm = cm.astype(&#39;float&#39;) / np.sum(cm, axis=1, keepdims=True)
        print(np.around(cm, decimals=2))
        print(&#34;Classification Report :&#34;)
        report = classification_report(y_true, y_pred, digits=4, zero_division=0)
        report = report.replace(&#39;\n\n&#39;, &#39;\n&#39;)
        print(report, end=&#39;&#39;)</code></pre>
</details>
</dd>
<dt id="utils_nlp.get_report"><code class="name flex">
<span>def <span class="ident">get_report</span></span>(<span>y_true, y_pred, classes)</span>
</code></dt>
<dd>
<div class="desc"><p>This function parses the classification report given by sklearn to
get all the row names metric values as floats and supports for each
class label.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y_true</code></strong></dt>
<dd>true (numerical) labels of data</dd>
<dt><strong><code>y_pred</code></strong></dt>
<dd>predicted (numerical) labels of the same data</dd>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>class_names</code></dt>
<dd>a python list of class labels (here, row names from report)</dd>
<dt><code>plotMat</code></dt>
<dd>numerical values (metrics) in the classification report</dd>
<dt><code>support</code></dt>
<dd>the number of instances for each class_name present in report</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_report(y_true, y_pred, classes):
        &#34;&#34;&#34;This function parses the classification report given by sklearn to
        get all the row names metric values as floats and supports for each
        class label.
        Args:
                y_true: true (numerical) labels of data
                y_pred: predicted (numerical) labels of the same data
                classes: a python list of class labels
        Returns:
                class_names: a python list of class labels (here, row names from report)
                plotMat: numerical values (metrics) in the classification report
                support: the number of instances for each class_name present in report
        &#34;&#34;&#34;
        clf_report = classification_report(y_true, y_pred, digits=4, labels=classes, zero_division=0)
        clf_report = clf_report.replace(&#39;\n\n&#39;, &#39;\n&#39;)
        clf_report = clf_report.replace(&#39;micro avg&#39;, &#39;micro_avg&#39;)
        clf_report = clf_report.replace(&#39;macro avg&#39;, &#39;macro_avg&#39;)
        clf_report = clf_report.replace(&#39;weighted avg&#39;, &#39;weighted_avg&#39;)
        clf_report = clf_report.replace(&#39; / &#39;, &#39;/&#39;)
        lines = clf_report.split(&#39;\n&#39;)

        class_names, plotMat, support = [], [], []
        for line in lines[1:]:
                t = line.strip().split()
                if len(t) &lt; 2:
                        continue
                v = [float(x) for x in t[1: len(t) - 1]]
                if len(v) == 1 : v = v * 3
                support.append(int(t[-1]))
                class_names.append(t[0])
                plotMat.append(v)
        plotMat = np.array(plotMat)
        support = np.array(support)
        return class_names, plotMat, support</code></pre>
</details>
</dd>
<dt id="utils_nlp.get_scores"><code class="name flex">
<span>def <span class="ident">get_scores</span></span>(<span>y_true, y_pred, classes)</span>
</code></dt>
<dd>
<div class="desc"><p>This function calculates the correct and incorrect counts for each label
as a fraction to the total instances of that class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y_true</code></strong></dt>
<dd>true (numerical) labels of data</dd>
<dt><strong><code>y_pred</code></strong></dt>
<dd>predicted (numerical) labels of the same data</dd>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>numpy array of tuple of (correct,incorrect) fractions for each class</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_scores(y_true, y_pred, classes):
        &#34;&#34;&#34;This function calculates the correct and incorrect counts for each label
        as a fraction to the total instances of that class.
        Args:
                y_true: true (numerical) labels of data
                y_pred: predicted (numerical) labels of the same data
                classes: a python list of class labels
        Returns:
                numpy array of tuple of (correct,incorrect) fractions for each class
        &#34;&#34;&#34;
        correct, wrong = {}, {}
        for tag in classes:
                correct[tag] = 0
                wrong[tag] = 0
                
        for tag, pred in zip(y_true, y_pred):
                if tag == pred:
                        correct[tag] += 1
                else:
                        wrong[tag] += 1
                        
        scores = []
        total = len(y_true)
        for tag in classes:
                cur = np.array([correct[tag], wrong[tag]])
                scores.append(cur / total)
        return np.array(scores)</code></pre>
</details>
</dd>
<dt id="utils_nlp.plot_clf_report"><code class="name flex">
<span>def <span class="ident">plot_clf_report</span></span>(<span>classes, plotMat, support, cmap=&lt;matplotlib.colors.LinearSegmentedColormap object&gt;, filename='classification_report')</span>
</code></dt>
<dd>
<div class="desc"><p>This function plots the classification report as an image, using the
parsed values from the sklearn classification report and saves the image in
the current working directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
<dt><strong><code>plotMat</code></strong></dt>
<dd>numerical values (metrics) in the classification report</dd>
<dt><strong><code>support</code></strong></dt>
<dd>the number of instances for each class present in report</dd>
<dt><strong><code>cmap</code></strong></dt>
<dd>the color map to be used in the output image</dd>
<dt><strong><code>filename</code></strong></dt>
<dd>the filename with which the plot will be saved (can be a path too)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>No return value. Shows and saves the report image.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues, filename=&#39;classification_report&#39;):
        &#34;&#34;&#34;This function plots the classification report as an image, using the 
        parsed values from the sklearn classification report and saves the image in 
        the current working directory.
        Args:
                classes: a python list of class labels
                plotMat: numerical values (metrics) in the classification report
                support: the number of instances for each class present in report
                cmap: the color map to be used in the output image
                filename: the filename with which the plot will be saved (can be a path too)
        Returns:
                No return value. Shows and saves the report image.
        &#34;&#34;&#34;
        k = len(classes)
        title = &#39;Classification Report&#39;
        xticklabels = [&#39;Precision&#39;, &#39;Recall&#39;, &#39;F1-score&#39;]
        yticklabels = [&#39;{0} ({1})&#39;.format(classes[idx], sup) for idx, sup in enumerate(support)]
        plt.clf()
        fig, ax = plt.subplots(figsize=(22,12))
        ax.set_title(title, y=-0.06, fontsize=22)
        ax.xaxis.set_ticks_position(&#39;top&#39;)
        ax.xaxis.set_label_position(&#39;top&#39;)
        ax.xaxis.set_tick_params(labelsize=18)
        ax.yaxis.set_tick_params(labelsize=14)
        plt.imshow(plotMat, interpolation=&#39;nearest&#39;, cmap=cmap, aspect=&#39;auto&#39;)
        plt.clim(vmin=0.0, vmax=1.0)
        cbar = plt.colorbar()
        cbar.ax.tick_params(labelsize=80/k) 
        plt.xticks(np.arange(3), xticklabels, rotation=0)
        plt.yticks(np.arange(len(classes)), yticklabels)
        thresh = np.max(plotMat) / 2
        thresh = 1 / 2
        for i in range(plotMat.shape[0]):
                for j in range(plotMat.shape[1]):
                        color = &#34;white&#34; if (plotMat[i, j] &gt; thresh) else &#34;black&#34;
                        plt.text(j, i, format(plotMat[i, j], &#39;.2f&#39;), horizontalalignment=&#34;center&#34;, color=color, fontsize=100/k)

        plt.xlabel(&#39;Metrics&#39;,fontsize=22)
        plt.ylabel(&#39;Classes&#39;,fontsize=22)
        plt.tight_layout()
        plt.savefig(filename+&#39;.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)</code></pre>
</details>
</dd>
<dt id="utils_nlp.plot_confusion_matrix"><code class="name flex">
<span>def <span class="ident">plot_confusion_matrix</span></span>(<span>classes, mat, normalize=True, cmap=&lt;matplotlib.colors.LinearSegmentedColormap object&gt;, filename='confusion_matrix')</span>
</code></dt>
<dd>
<div class="desc"><p>This function plots the confusion matrix as an image, using the
parsed values from the confusion matrix and saves the image in
the current working directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
<dt><strong><code>mat</code></strong></dt>
<dd>numerical values (metrics) in the confusion matrix</dd>
<dt><strong><code>normalize</code></strong></dt>
<dd>controls the normalization of the confusion
matrix (rows sum to 1 or not)</dd>
<dt><strong><code>cmap</code></strong></dt>
<dd>the color map to be used in the output image</dd>
<dt><strong><code>filename</code></strong></dt>
<dd>the filename with which the plot will be saved (can be a path too)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>No return value. Shows and saves the confusion matrix image.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues, filename=&#39;confusion_matrix&#39;):
        &#34;&#34;&#34;This function plots the confusion matrix as an image, using the 
        parsed values from the confusion matrix and saves the image in 
        the current working directory.
        Args:
                classes: a python list of class labels
                mat: numerical values (metrics) in the confusion matrix
                normalize: controls the normalization of the confusion 
                        matrix (rows sum to 1 or not)
                cmap: the color map to be used in the output image
                filename: the filename with which the plot will be saved (can be a path too)
        Returns:
                No return value. Shows and saves the confusion matrix image.
        &#34;&#34;&#34;
        k = len(classes)
        cm = np.copy(mat)
        title = &#39;Confusion Matrix (without normalization)&#39;
        if normalize:
                cm = cm.astype(&#39;float&#39;) / np.sum(cm, axis=1, keepdims=True)
                title = title.replace(&#39;without&#39;, &#39;with&#39;)
        plt.clf()    
        fig, ax = plt.subplots(figsize=(20,10))
        ax.set_title(title, y=-0.06, fontsize=66/k)
        ax.xaxis.set_ticks_position(&#39;top&#39;)
        ax.xaxis.set_label_position(&#39;top&#39;)
        plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap)
        plt.clim(vmin=0.0, vmax=1.0)
        cbar = plt.colorbar()
        cbar.ax.tick_params(labelsize=40/k) 
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.yticks(tick_marks, classes)
        fmt = &#39;.2f&#39; if normalize else &#39;d&#39;
        thresh = np.max(cm) / 2
        thresh = 1 / 2
        for i in range(cm.shape[0]):
                for j in range(cm.shape[1]):
                        color = &#34;white&#34; if (cm[i, j] &gt; thresh) else &#34;black&#34;
                        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&#34;center&#34;, color=color, fontsize=50/k)
        plt.ylabel(&#39;True label&#39;,fontsize=22)
        plt.xlabel(&#39;Predicted label&#39;, fontsize=22)
        plt.tight_layout()
        plt.savefig(filename+&#39;.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)</code></pre>
</details>
</dd>
<dt id="utils_nlp.plot_tag_scores"><code class="name flex">
<span>def <span class="ident">plot_tag_scores</span></span>(<span>classes, scores, filename='tag_scores')</span>
</code></dt>
<dd>
<div class="desc"><p>This function plots the histogram for tag scores and saves the image in
the current working directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>classes</code></strong></dt>
<dd>a python list of class labels</dd>
<dt><strong><code>scores</code></strong></dt>
<dd>a dictionary of correct and incorrect counts for each label</dd>
<dt><strong><code>filename</code></strong></dt>
<dd>the filename with which the plot will be saved (can be a path too)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>No return value. Shows and saves the tag scores plot.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_tag_scores(classes, scores, filename=&#39;tag_scores&#39;):
        &#34;&#34;&#34;This function plots the histogram for tag scores and saves the image in 
        the current working directory.
        Args:
                classes: a python list of class labels
                scores: a dictionary of correct and incorrect counts for each label
                filename: the filename with which the plot will be saved (can be a path too)
        Returns:
                No return value. Shows and saves the tag scores plot.
        &#34;&#34;&#34;
        plt.clf()
        width = 0.45
        fig, ax = plt.subplots(figsize=(20,10))
        ax.xaxis.set_tick_params(labelsize=18, rotation=25)
        ax.yaxis.set_tick_params(labelsize=18)
        range_bar1 = np.arange(len(classes))
        rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color=&#39;b&#39;)
        rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color=&#39;r&#39;)

        ax.set_ylabel(&#39;Scores&#39;,fontsize=22)
        ax.set_title(&#39;Tag scores&#39;, fontsize=22)
        ax.set_xticks(range_bar1 + width / 2)
        ax.set_xticklabels(classes)

        ax.legend((rects1[0], rects2[0]), (&#39;Correct&#39;, &#39;Wrong&#39;), fontsize=20)
        plt.legend()
        plt.savefig(filename+&#39;.png&#39;, bbox_inches=&#34;tight&#34;, transparent=True)
        plt.show()</code></pre>
</details>
</dd>
<dt id="utils_nlp.re_label"><code class="name flex">
<span>def <span class="ident">re_label</span></span>(<span>y)</span>
</code></dt>
<dd>
<div class="desc"><p>re_label function takes in input a list/array of any hashable
data type, for example, strings, integers etc, which represent class labels.
This function replaces all the class labels with integers 0 to K-1 where
K is the number of classes (distinct elements present in input).
Args:
y: python list/numpy array with each element a hashable data type
Returns:
relabeled numpy array in integer format</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def re_label(y):
        &#34;&#34;&#34;re_label function takes in input a list/array of any hashable
        data type, for example, strings, integers etc, which represent class labels.
        This function replaces all the class labels with integers 0 to K-1 where
        K is the number of classes (distinct elements present in input).
        Args: 
                y: python list/numpy array with each element a hashable data type
        Returns: 
                relabeled numpy array in integer format
        &#34;&#34;&#34;
        labels = sorted(list(set(list(y))))
        inv = {v:i for i,v in enumerate(labels)}
        return np.array([inv[v] for v in y], dtype=np.int32)</code></pre>
</details>
</dd>
<dt id="utils_nlp.remove_labels"><code class="name flex">
<span>def <span class="ident">remove_labels</span></span>(<span>X, y, labels_to_remove)</span>
</code></dt>
<dd>
<div class="desc"><p>remove_labels does exactly what the name suggests. This function removes
instances from both X and y where that instance has a label which is provided
in the argument labels_to_remove. Instances refer to rows in the data X and
labels y.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong></dt>
<dd>data having same number of instances as y</dd>
<dt><strong><code>y</code></strong></dt>
<dd>numpy array of labels corresponding to the feature matrix X
labels_to_remove: a list of labels which will be removed from the data
as well as label list.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>X_new</code></dt>
<dd>X with some removed rows</dd>
<dt><code>y_new</code></dt>
<dd>y with some removed rows</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_labels(X, y, labels_to_remove):
        &#34;&#34;&#34;remove_labels does exactly what the name suggests. This function removes
        instances from both X and y where that instance has a label which is provided
        in the argument labels_to_remove. Instances refer to rows in the data X and 
        labels y.
        Args:
                X: data having same number of instances as y
                y: numpy array of labels corresponding to the feature matrix X
                        labels_to_remove: a list of labels which will be removed from the data
                        as well as label list.
        Returns:
                X_new: X with some removed rows
                y_new: y with some removed rows
        &#34;&#34;&#34;
        indices = []
        for i in range(y.shape[0]):
                if y[i] in labels_to_remove:
                        indices.append(i)
        if isinstance(X, list):
                X_new = [np.delete(X[i], indices, axis=0) for i in range(len(X))]
        else:
                X_new = np.delete(X, indices, axis=0)
        y_new = np.delete(y, indices, axis=0)
        return X_new, y_new</code></pre>
</details>
</dd>
<dt id="utils_nlp.round_label"><code class="name flex">
<span>def <span class="ident">round_label</span></span>(<span>a, limit=1)</span>
</code></dt>
<dd>
<div class="desc"><p>round_label rounds off labels where the label list actually is a continuous
real value depicting the intensity of the sentiment.
Negative values are rounded off to nearest smaller integer
Positive values are rounded off to nearest larger integer
The output is constrained to be in the range [-limit,limit]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>real number (treated as intensity)</dd>
<dt><strong><code>limit</code></strong></dt>
<dd>argument for constraining the output</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>rounded off value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def round_label(a, limit=1):
        &#34;&#34;&#34;round_label rounds off labels where the label list actually is a continuous
        real value depicting the intensity of the sentiment. 
        Negative values are rounded off to nearest smaller integer
        Positive values are rounded off to nearest larger integer 
        The output is constrained to be in the range [-limit,limit]
        Args:
                a: real number (treated as intensity)
                limit: argument for constraining the output
        Returns:
                rounded off value
        &#34;&#34;&#34;
        if int(a) == a:
                res = int(a)
        elif a &gt; 0:
                res = int(a) + 1
        else:
                res = int(a) - 1
        res = max(res, -limit)
        res = min(res, limit)
        return res</code></pre>
</details>
</dd>
<dt id="utils_nlp.to_categorical"><code class="name flex">
<span>def <span class="ident">to_categorical</span></span>(<span>y, num_classes=None, dtype='float32')</span>
</code></dt>
<dd>
<div class="desc"><p>to_categorical converts labels to their one-hot encoded representation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y</code></strong></dt>
<dd>array/list of labels within range 0 to K-1 where K &lt;= num_classes</dd>
<dt><strong><code>num_classes</code></strong></dt>
<dd>the number of classes to encoded, if None, max(y)+1 is used</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>dtype of the output array</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>one-hot encoded vector of shape NxK where N is number of examples and K is
number of classes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_categorical(y, num_classes=None, dtype=&#39;float32&#39;):
        &#34;&#34;&#34;to_categorical converts labels to their one-hot encoded representation.
        Args:
                y: array/list of labels within range 0 to K-1 where K &lt;= num_classes
                num_classes: the number of classes to encoded, if None, max(y)+1 is used
                dtype: dtype of the output array
        Returns:
                one-hot encoded vector of shape NxK where N is number of examples and K is 
                number of classes.
        &#34;&#34;&#34;
        y = np.array(y, dtype=&#39;int&#39;)
        input_shape = y.shape
        if input_shape and input_shape[-1] == 1 and len(input_shape) &gt; 1:
                input_shape = tuple(input_shape[:-1])
        y = y.ravel()
        if not num_classes:
                num_classes = np.max(y) + 1
        n = y.shape[0]
        categorical = np.zeros((n, num_classes), dtype=dtype)
        categorical[np.arange(n), y] = 1
        output_shape = input_shape + (num_classes,)
        categorical = np.reshape(categorical, output_shape)
        return categorical</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="utils_nlp.calc_test_result" href="#utils_nlp.calc_test_result">calc_test_result</a></code></li>
<li><code><a title="utils_nlp.get_report" href="#utils_nlp.get_report">get_report</a></code></li>
<li><code><a title="utils_nlp.get_scores" href="#utils_nlp.get_scores">get_scores</a></code></li>
<li><code><a title="utils_nlp.plot_clf_report" href="#utils_nlp.plot_clf_report">plot_clf_report</a></code></li>
<li><code><a title="utils_nlp.plot_confusion_matrix" href="#utils_nlp.plot_confusion_matrix">plot_confusion_matrix</a></code></li>
<li><code><a title="utils_nlp.plot_tag_scores" href="#utils_nlp.plot_tag_scores">plot_tag_scores</a></code></li>
<li><code><a title="utils_nlp.re_label" href="#utils_nlp.re_label">re_label</a></code></li>
<li><code><a title="utils_nlp.remove_labels" href="#utils_nlp.remove_labels">remove_labels</a></code></li>
<li><code><a title="utils_nlp.round_label" href="#utils_nlp.round_label">round_label</a></code></li>
<li><code><a title="utils_nlp.to_categorical" href="#utils_nlp.to_categorical">to_categorical</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>